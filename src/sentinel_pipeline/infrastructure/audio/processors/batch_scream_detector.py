import asyncio
import numpy as np
import torch
import torchaudio
import logging
import time
from typing import List, Tuple, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class BatchScreamDetector:
    """
    [GPU-Optimized Async Scream Detector]
    - Async IO: Non-blocking predict interface
    - Dynamic Batching: Collects requests into batches for GPU processing
    - GPU Preprocessing: torchaudio based MelSpectrogram & Normalization
    """

    def __init__(
        self, 
        model_path: str,
        threshold: float = 0.7,
        device: str = None, 
        batch_size: int = 16, 
        latency_limit: float = 0.05
    ):
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.latency_limit = latency_limit 
        self.model_path = model_path
        self.threshold = threshold
        
        # ìƒíƒœ ê´€ë¦¬
        self.queue: Optional[asyncio.Queue] = None
        self.worker_task: Optional[asyncio.Task] = None
        self.running = False
        self.loop: Optional[asyncio.AbstractEventLoop] = None
        
        # ëª¨ë¸ ë° ì „ì²˜ë¦¬ (Lazy Loading)
        self.mel_transform = None
        self.model = None
        
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.sample_rate = 16000
        self.n_mels = 128
        self.n_fft = 2048
        self.hop_length = 512
        self.target_length = 32000 # 2ì´ˆ

        # ë¦¬ì†ŒìŠ¤ ë¡œë“œ
        self._load_resources()

    def _load_resources(self):
        """ëª¨ë¸ ë° ì „ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ë¡œë“œ (ë™ê¸° ì‹¤í–‰)"""
        gpu_info = f" (Device: {torch.cuda.get_device_name(0)})" if "cuda" in self.device else ""
        logger.info(f"ğŸ”¥ [BatchScreamDetector] Loading resources on {self.device}{gpu_info}...")
        
        try:
            # 1. ì „ì²˜ë¦¬ ëª¨ë“ˆ (GPU)
            self.mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                n_mels=self.n_mels,
                power=2.0,
                norm='slaney',     # librosa í˜¸í™˜
                mel_scale='slaney' # librosa í˜¸í™˜
            ).to(self.device)
            
            # 2. ëª¨ë¸ ë¡œë“œ (resnet18 ê¸°ë°˜)
            from torchvision.models import resnet18
            import torch.nn as nn
            
            model = resnet18(weights=None) # Weights=None for clean load
            model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
            num_ftrs = model.fc.in_features
            model.fc = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(num_ftrs, 2)
            )
            
            if Path(self.model_path).exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
                new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
                model.load_state_dict(new_state_dict)
                logger.info(f"âœ… Model weights loaded from {self.model_path}")
            else:
                logger.warning(f"âš ï¸ Model path not found: {self.model_path}. Using random weights.")
            
            model.to(self.device)
            model.eval()
            self.model = model
            
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            raise e

    def start(self, loop: Optional[asyncio.AbstractEventLoop] = None):
        """ì›Œì»¤ ì‹œì‘ (ëª…ì‹œì  ë£¨í”„ ì§€ì • ê°€ëŠ¥)"""
        if not self.running:
            self.running = True
            self.loop = loop or asyncio.get_running_loop()
            
            # í ìƒì„±ì€ ë°˜ë“œì‹œ ë£¨í”„ ë‚´ì—ì„œ (ë˜ëŠ” ë£¨í”„ ë°”ì¸ë”©)
            async def _init_queue():
                self.queue = asyncio.Queue()
                self.worker_task = asyncio.create_task(self._worker_loop())
            
            if self.loop.is_running():
                asyncio.run_coroutine_threadsafe(_init_queue(), self.loop)
            else:
                # ì•„ì§ ë£¨í”„ê°€ ëŒì§€ ì•ŠëŠ”ë‹¤ë©´ (ì˜ˆ: í…ŒìŠ¤íŠ¸ ì½”ë“œ)
                # ì´ ë°©ì‹ì€ uvicorn í™˜ê²½ì—ì„  ê±°ì˜ ì•ˆ ì“°ì„
                pass
                
            logger.info(f"ğŸš€ Batch worker started on loop {id(self.loop)}")

    async def predict(self, audio: np.ndarray) -> dict:
        """[Public API]"""
        if not self.running or self.queue is None:
            # ë°©ì–´ ì½”ë“œ: ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê²°ê³¼ ì¦‰ì‹œ ë°˜í™˜ (ë˜ëŠ” ì—ëŸ¬)
            return {"is_scream": False, "prob": 0.0, "status": "not_ready"}

        future = self.loop.create_future()
        await self.queue.put((audio, future))
        return await future

    async def _worker_loop(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ë£¨í”„"""
        while self.running:
            batch_items = []
            try:
                # 1. ì²« ì•„ì´í…œ ëŒ€ê¸° (Timeout ì—†ìŒ = CPU Idle)
                item = await self.queue.get()
                
                # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
                if item is None:
                    break
                    
                batch_items.append(item)
                
                # 2. Latency Limit ë™ì•ˆ ì¶”ê°€ ìˆ˜ì§‘
                start_t = time.monotonic()
                while len(batch_items) < self.batch_size:
                    remaining = self.latency_limit - (time.monotonic() - start_t)
                    if remaining <= 0: break
                    
                    try:
                        item = await asyncio.wait_for(self.queue.get(), timeout=remaining)
                        # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
                        if item is None:
                            break
                        batch_items.append(item)
                    except asyncio.TimeoutError:
                        break
                
                # 3. ì²˜ë¦¬
                await self._process_batch(batch_items)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Worker Loop Error: {e}", exc_info=True)

    async def _process_batch(self, items: List[Tuple[np.ndarray, asyncio.Future]]):
        if not items: return
        
        futures = [x[1] for x in items]
        audios = [x[0] for x in items]
        
        try:
            # A. Numpy -> Tensor (CPU) -> GPU Stack
            tensors = []
            for a in audios:
                if len(a) < self.target_length:
                    a = np.pad(a, (0, self.target_length - len(a)), 'constant')
                else:
                    a = a[:self.target_length]
                tensors.append(torch.from_numpy(a))
            
            batch_tensor = torch.stack(tensors).float().to(self.device)
            
            # B. Preprocessing (GPU)
            with torch.no_grad():
                melspec = self.mel_transform(batch_tensor)
                
                # PowerToDB & Norm
                melspec_db = 10.0 * torch.log10(melspec + 1e-6)
                
                # Per-sample Max/Min
                flat = melspec_db.view(melspec_db.size(0), -1)
                max_val = flat.max(dim=1, keepdim=True)[0].view(-1, 1, 1)
                min_val = flat.min(dim=1, keepdim=True)[0].view(-1, 1, 1)
                
                denom = max_val - min_val
                denom[denom == 0] = 1.0
                
                spec_norm = (melspec_db - min_val) / denom
                input_tensor = spec_norm.unsqueeze(1) # [B, 1, H, W]
                
                # C. Inference
                outputs = self.model(input_tensor)
                probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()
            
            # D. Result
            for i, prob in enumerate(probs):
                if not futures[i].done():
                    futures[i].set_result({
                        "is_scream": float(prob) > self.threshold,
                        "prob": float(prob)
                    })
                    
        except Exception as e:
            logger.error(f"Batch Processing Error: {e}", exc_info=True)
            for f in futures:
                if not f.done(): f.set_exception(e)
    
    async def stop(self):
        """ì›Œì»¤ ì¢…ë£Œ"""
        if not self.running:
            return
        
        self.running = False
        
        # íì— Noneì„ ë„£ì–´ì„œ ì›Œì»¤ ë£¨í”„ ì¢…ë£Œ ì‹ í˜¸
        if self.queue:
            await self.queue.put(None)
        
        # ì›Œì»¤ íƒœìŠ¤í¬ ì¢…ë£Œ ëŒ€ê¸°
        if self.worker_task:
            try:
                await asyncio.wait_for(self.worker_task, timeout=2.0)
            except asyncio.TimeoutError:
                logger.warning("Worker task did not stop in time, cancelling...")
                self.worker_task.cancel()
                try:
                    await self.worker_task
                except asyncio.CancelledError:
                    pass
        
        logger.info("BatchScreamDetector stopped")