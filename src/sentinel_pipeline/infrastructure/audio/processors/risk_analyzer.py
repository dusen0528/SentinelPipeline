"""
ìœ„í—˜ í‚¤ì›Œë“œ ë¶„ì„ í”„ë¡œì„¸ì„œ (Producer-Consumer Pattern ì ìš©)

- Producer: RiskAnalyzer (ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ íì— ì œì¶œ)
- Consumer: GlobalInferenceEngine (ë°±ê·¸ë¼ìš´ë“œì—ì„œ Whisper ì¶”ë¡  ìˆ˜í–‰)
- Queue: Python native queue (ê°€ì¥ ë¹ ë¦„)
"""

import threading
import queue
import logging
import time
from pathlib import Path
from typing import Any, Optional, Callable, Dict
from dataclasses import dataclass

import numpy as np

from sentinel_pipeline.domain.interfaces.audio_processor import AudioProcessor
from sentinel_pipeline.infrastructure.audio.processors.hybrid_keyword_detector import (
    HybridKeywordDetector,
)

logger = logging.getLogger(__name__)

# --- [Data Structure] ---
@dataclass
class InferenceRequest:
    """íì— ë“¤ì–´ê°ˆ ì‘ì—… ë‹¨ìœ„"""
    stream_id: str
    audio_data: np.ndarray
    callback: Optional[Callable[[Dict[str, Any]], None]]
    timestamp: float

# --- [Consumer: The GPU Worker] ---
class GlobalInferenceEngine:
    """
    [Singleton] ì¤‘ì•™ ì¶”ë¡  ì—”ì§„
    ëª¨ë¸ ë¡œë”©, í ê´€ë¦¬, ë°±ê·¸ë¼ìš´ë“œ ì¶”ë¡ ì„ ì „ë‹´í•©ë‹ˆë‹¤.
    """
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(GlobalInferenceEngine, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        # 1. ì„¤ì • (ì‚¬ë ¹ê´€ë‹˜ì˜ RTX 2000ì„ ìœ„í•´ Small ê³ ì • ì¶”ì²œ)
        self.model_size = "small"
        self.device = "cuda"
        self.compute_type = "float16"
        
        # 2. í ìƒì„± (Backpressure ì¡°ì ˆìš© maxsize ì„¤ì •)
        # ë„ˆë¬´ ë§ì´ ìŒ“ì´ë©´(100ê°œ ì´ìƒ) ìµœì‹  ë°ì´í„°ë¥¼ ìœ„í•´ ì˜¤ë˜ëœê±´ ë²„ë¦¬ê±°ë‚˜ ì…ë ¥ì„ ë§‰ì•„ì•¼ í•¨
        self.queue = queue.Queue(maxsize=100)
        self.running = True
        
        # 3. ëª¨ë¸ ë° ê°ì§€ê¸° ì´ˆê¸°í™” (Lazy Loading)
        self.model = None
        self.detector = None
        self._load_resources()

        # 4. ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ (ì†Œë¹„ì)
        self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True, name="GPU-Inference-Worker")
        self.worker_thread.start()
        
        self._initialized = True
        logger.info(f"ğŸš€ [GlobalInferenceEngine] ì—”ì§„ ì‹œë™ ì™„ë£Œ (Queue Size: 100)")

    def _load_resources(self):
        """ëª¨ë¸ê³¼ í‚¤ì›Œë“œ ê°ì§€ê¸°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤."""
        try:
            from faster_whisper import WhisperModel
            import torch
            
            # device ì¬í™•ì¸
            if self.device == "cuda" and not torch.cuda.is_available():
                self.device = "cpu"
                self.compute_type = "int8"

            logger.info(f"ğŸ“¥ [Engine] Whisper ëª¨ë¸ ë¡œë”© ì‹œì‘ ({self.model_size} / {self.device})...")
            self.model = WhisperModel(
                self.model_size, 
                device=self.device, 
                compute_type=self.compute_type
            )
            logger.info("âœ… [Engine] Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

            # í‚¤ì›Œë“œ ê°ì§€ê¸°ëŠ” RiskAnalyzerì—ì„œ ì„¤ì •ì„ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, 
            # Singleton êµ¬ì¡°ìƒ ì—¬ê¸°ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê±°ë‚˜, 
            # ìš”ì²­ ì‹œ detectorë¥¼ ì¸ìë¡œ ë°›ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (í•„ìš”ì‹œ config ì£¼ì… êµ¬ì¡°ë¡œ ë³€ê²½ ê°€ëŠ¥)
            self.detector = HybridKeywordDetector(
                enable_medium_path=True,
                enable_heavy_path=True,
                heavy_path_async=True,
                semantic_threshold=0.7,
                use_korean_model=False
            )
            logger.info("âœ… [Engine] í‚¤ì›Œë“œ ê°ì§€ê¸° ì¤€ë¹„ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ [Engine] ë¦¬ì†ŒìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    def submit(self, request: InferenceRequest):
        """[Producer Interface] ì‘ì—…ì„ íì— ë„£ìŠµë‹ˆë‹¤. (Non-blocking)"""
        try:
            # íê°€ ê½‰ ì°¼ìœ¼ë©´ ì¦‰ì‹œ ì—ëŸ¬ ë°œìƒ (ì˜¤ë˜ëœ ìš”ì²­ ëŒ€ê¸°ì‹œí‚¤ì§€ ì•Šê³  ë²„ë¦¼ -> ì‹¤ì‹œê°„ì„± ìœ ì§€)
            self.queue.put_nowait(request)
        except queue.Full:
            # ë¡œê¹…ì€ ë„ˆë¬´ ë§ì´ ì°í ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒ˜í”Œë§í•˜ê±°ë‚˜ debug ë ˆë²¨ë¡œ
            # logger.warning(f"âš ï¸ [Engine] íê°€ ê°€ë“ ì°¼ìŠµë‹ˆë‹¤. ìš”ì²­ ë“œë: {request.stream_id}")
            pass

    def _worker_loop(self):
        """[Consumer Loop] íì—ì„œ í•˜ë‚˜ì”© êº¼ë‚´ ì²˜ë¦¬"""
        logger.info("ğŸ”§ [Worker] ì¶”ë¡  ë£¨í”„ ì‹œì‘")
        
        while self.running:
            try:
                # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (ëŒ€ê¸°)
                req = self.queue.get()
                
                # ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
                start_time = time.time()
                
                # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ì—¬ê¸°ì„œ ìˆ˜í–‰í•˜ì—¬ Main Thread ë¶€í•˜ ê°ì†Œ)
                if req.audio_data.dtype != np.float32:
                    audio = req.audio_data.astype(np.float32)
                else:
                    audio = req.audio_data
                
                # 2. Whisper ì¶”ë¡  (GPU ì‚¬ìš©)
                segments, _ = self.model.transcribe(
                    audio,
                    beam_size=1, # ì†ë„ ìµœì í™”
                    language="ko",
                    vad_filter=True, # Whisper ë‚´ë¶€ VADë„ ì¼œë‘  (ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
                    vad_parameters=dict(min_silence_duration_ms=200, threshold=0.3)
                )
                
                full_text = " ".join([s.text for s in segments]).strip()
                
                # 3. í‚¤ì›Œë“œ ë¶„ì„
                result_data = {
                    "text": full_text,
                    "is_dangerous": False,
                    "event_type": None,
                    "keyword": None,
                    "confidence": 0.0,
                    "stream_id": req.stream_id,
                    "latency": time.time() - req.timestamp
                }

                if full_text and self.detector:
                    analysis = self.detector.analyze(full_text)
                    result_data.update(analysis) # ê²°ê³¼ ë³‘í•©

                # 4. ì½œë°± ì‹¤í–‰ (ê²°ê³¼ í†µë³´)
                if req.callback:
                    try:
                        req.callback(result_data)
                    except Exception as cb_err:
                        logger.error(f"âŒ [Worker] ì½œë°± ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {cb_err}")

                # ì‘ì—… ì™„ë£Œ í‘œì‹œ
                self.queue.task_done()
                
                # (ì„ íƒ) ì²˜ë¦¬ ì†ë„ ë¡œê¹…
                # logger.debug(f"âš¡ ì²˜ë¦¬ì™„ë£Œ: {req.stream_id} (len={len(full_text)}) time={time.time()-start_time:.3f}s")

            except Exception as e:
                logger.error(f"âŒ [Worker] ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë£¨í”„ëŠ” ê³„ì† ëŒì•„ì•¼ í•¨

# --- [Producer: The Client] ---
class RiskAnalyzer(AudioProcessor):
    """
    ì´ì œ RiskAnalyzerëŠ” ì§ì ‘ ë¬´ê±°ìš´ ì¼ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ì—”ì§„(GlobalInferenceEngine)ì— ì‘ì—…ì„ ë˜ì ¸ì£¼ëŠ” ì—­í• ë§Œ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, stream_id: str = "unknown", **kwargs):
        # **kwargsëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ë°›ì•„ì£¼ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì—”ì§„ì´ ê´€ë¦¬í•©ë‹ˆë‹¤.
        self.stream_id = stream_id
        self.engine = GlobalInferenceEngine() # Singleton ì¸ìŠ¤í„´ìŠ¤ íšë“

    def process(self, audio_data: np.ndarray, callback: Optional[Callable] = None) -> dict[str, Any]:
        """
        [ë¹„ë™ê¸° ì²˜ë¦¬ ë³€ê²½]
        ì´ì œ ê²°ê³¼ë¥¼ ë°”ë¡œ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (None ë°˜í™˜).
        ëŒ€ì‹  callback í•¨ìˆ˜ë¥¼ í†µí•´ ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        """
        if len(audio_data) == 0:
            return {}

        # 1. ìš”ì²­ ê°ì²´ ìƒì„±
        request = InferenceRequest(
            stream_id=self.stream_id,
            audio_data=np.copy(audio_data), # ë°ì´í„° ë³µì‚¬ ì¤‘ìš” (ì›ë³¸ì´ ë®ì–´ì¨ì§ˆ ìˆ˜ ìˆìŒ)
            callback=callback,
            timestamp=time.time()
        )
        
        # 2. ì—”ì§„ì— ì œì¶œ (Non-blocking)
        self.engine.submit(request)
        
        # 3. ì¦‰ì‹œ ë¦¬í„´ (ë©”ì¸ ìŠ¤ë ˆë“œ í•´ë°©!)
        # ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜í•˜ì§€ë§Œ,
        # í˜¸ì¶œí•˜ëŠ” ìª½(AudioManager)ì—ì„œ ë¦¬í„´ê°’ì„ ê¸°ë‹¤ë¦¬ë©´ ì•ˆ ë©ë‹ˆë‹¤.
        return {
            "status": "queued",
            "is_dangerous": False # ì„ì‹œ ê°’
        }