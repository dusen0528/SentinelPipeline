"""
GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„°

ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ GPU ì²˜ë¦¬ ìš©ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ íë¦„:
1. Input: Nê°œì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ (sample_data/ í´ë”ì˜ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼)
2. Step 1 (Scream Detector): ResNet18 ëª¨ë¸ë¡œ ë¹„ëª… ê°ì§€ (GPU ìƒì‹œ ë¶€í•˜)
3. Step 2 (Logic): ë¹„ëª…ì´ ì•„ë‹Œ ê²½ìš°ë§Œ STT ì‹¤í–‰
4. Step 3 (STT Pipeline): Whisper ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ â†’ í‚¤ì›Œë“œ ë¶„ì„
5. Output: ì§€ì—° ì‹œê°„(Latency)ê³¼ GPU/CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ë§¤ ì²­í¬ë³„ ê¸°ë¡)

ì‚¬ìš©ë²•:
    python -m sentinel_pipeline.scripts.benchmark_runner --streams 10
    python -m sentinel_pipeline.scripts.benchmark_runner --streams 10 --continuous --duration 30
"""

import csv
import gc
import random
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Optional

import librosa
import numpy as np
import psutil
import torch

# ë¡œê¹… ì„¤ì •
from sentinel_pipeline.common.logging import get_logger

logger = get_logger(__name__)


@dataclass
class StreamMetrics:
    """ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ì˜ ì²˜ë¦¬ ê²°ê³¼ ë©”íŠ¸ë¦­ (ë§¤ ì²­í¬ë³„ ë¦¬ì†ŒìŠ¤ í¬í•¨)"""
    stream_id: int
    step1_latency: float  # ë¹„ëª… ê°ì§€ ì‹œê°„ (ì´ˆ)
    step2_latency: float  # STT ë³€í™˜ ì‹œê°„ (ì´ˆ, ì‹¤í–‰ ì•ˆ í–ˆìœ¼ë©´ 0)
    total_latency: float  # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    detected: bool        # ë¹„ëª… ê°ì§€ ì—¬ë¶€
    scream_prob: float = 0.0  # ë¹„ëª… í™•ë¥ 
    transcript: str = ""  # STT ê²°ê³¼ í…ìŠ¤íŠ¸
    audio_file: str = ""  # ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ íŒŒì¼ëª…
    audio_category: str = "normal"  # Ground Truth ì¹´í…Œê³ ë¦¬ (scream | emergency_keyword | normal)
    # ì²­í¬ ì²˜ë¦¬ ì‹œì ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ (ì‹œê³„ì—´ ë¶„ì„ìš©)
    gpu_memory_mb: float = 0.0  # í•´ë‹¹ ì²­í¬ ì²˜ë¦¬ ì‹œì ì˜ GPU ë©”ëª¨ë¦¬ (MB)
    cpu_percent: float = 0.0    # í•´ë‹¹ ì²­í¬ ì²˜ë¦¬ ì‹œì ì˜ CPU ì‚¬ìš©ë¥  (%)
    system_memory_mb: float = 0.0  # í•´ë‹¹ ì²­í¬ ì²˜ë¦¬ ì‹œì ì˜ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ (RAM, MB)


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    streams: int
    avg_latency: float
    max_latency: float
    min_latency: float
    fps: float
    gpu_memory_mb: float
    gpu_memory_peak_mb: float
    cpu_percent: float
    device: str
    scream_count: int
    stt_count: int
    total_time: float
    duration: float = 0.0  # í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)
    total_processed: int = 0  # ì´ ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜
    details: list = field(default_factory=list)
    
    def save_to_csv(self, filepath: Optional[str] = None) -> str:
        """
        ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filepath: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"benchmark_result_{timestamp}.csv"
        
        filepath = Path(filepath)
        
        with open(filepath, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            
            # í—¤ë” ì‘ì„±
            writer.writerow([
                "stream_id",
                "chunk_id",             # Continuous ëª¨ë“œì—ì„œ ëª‡ ë²ˆì§¸ ì²­í¬ì¸ì§€
                "timestamp",            # ì²˜ë¦¬ ì‹œì  (Unix timestamp)
                "audio_file",
                "audio_category",       # Ground Truth (GT): scream | emergency_keyword | normal
                "detected",             # ëª¨ë¸ ì˜ˆì¸¡ (ë¹„ëª… ê°ì§€ ì—¬ë¶€)
                "scream_prob",          # ë¹„ëª… í™•ë¥  (0~1)
                "step1_latency_ms",     # ë¹„ëª… ê°ì§€ ì‹œê°„ (ResNet)
                "step2_latency_ms",     # STT ë³€í™˜ ì‹œê°„ (Whisper)
                "total_latency_ms",     # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
                "gpu_memory_mb",        # ì²­í¬ ì²˜ë¦¬ ì‹œì  GPU ë©”ëª¨ë¦¬ (ì‹œê³„ì—´)
                "cpu_percent",          # ì²­í¬ ì²˜ë¦¬ ì‹œì  CPU ì‚¬ìš©ë¥  (ì‹œê³„ì—´)
                "system_memory_mb",     # ì²­í¬ ì²˜ë¦¬ ì‹œì  ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ (RAM, ì‹œê³„ì—´)
                "transcript",           # STT ì¶”ì¶œ í…ìŠ¤íŠ¸
            ])
            
            # ë°ì´í„° ì‘ì„±
            for d in self.details:
                writer.writerow([
                    d.get("stream_id", ""),
                    d.get("chunk_id", 0),
                    d.get("timestamp", ""),
                    d.get("audio_file", ""),
                    d.get("audio_category", "normal"),
                    d.get("detected", False),
                    d.get("scream_prob", 0),
                    d.get("step1_latency", 0),
                    d.get("step2_latency", 0),
                    d.get("total_latency", 0),
                    d.get("gpu_memory_mb", 0),
                    d.get("cpu_percent", 0),
                    d.get("system_memory_mb", 0),
                    d.get("transcript", ""),
                ])
        
        return str(filepath)


class LoadTestSimulator:
    """
    GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„°
    
    ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼(sample_data/)ì„ ì‚¬ìš©í•˜ì—¬ ScreamDetector + Whisper STT íŒŒì´í”„ë¼ì¸ì˜
    ì²˜ë¦¬ ìš©ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ë§¤ ì²­í¬ ì²˜ë¦¬ ì‹œì ì˜ GPU/CPU ë¦¬ì†ŒìŠ¤ë„ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    
    # ì˜¤ë””ì˜¤ ì„¤ì •
    SAMPLE_RATE = 16000
    WINDOW_SEC = 2.0  # 2ì´ˆ ìœˆë„ìš°
    
    def __init__(
        self,
        num_streams: int = 1,
        gpu_enabled: bool = True,
        scream_threshold: float = 0.7,
        whisper_model: str = "base",
        model_path: Optional[str] = None,
        sample_data_path: Optional[str] = None,
    ):
        """
        Args:
            num_streams: ì‹œë®¬ë ˆì´ì…˜í•  ìŠ¤íŠ¸ë¦¼ ê°œìˆ˜
            gpu_enabled: GPU ì‚¬ìš© ì—¬ë¶€
            scream_threshold: ë¹„ëª… íŒì • ì„ê³„ê°’ (ì´ ê°’ ì´ˆê³¼ ì‹œ ë¹„ëª…ìœ¼ë¡œ íŒì •)
            whisper_model: Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)
            model_path: ScreamDetector ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
            sample_data_path: ìƒ˜í”Œ ì˜¤ë””ì˜¤ íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.num_streams = num_streams
        self.scream_threshold = scream_threshold
        self.whisper_model_name = whisper_model
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if gpu_enabled and torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
            
        logger.info(f"LoadTestSimulator ì´ˆê¸°í™”: device={self.device}, streams={num_streams}")
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        if model_path is None:
            # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
            project_root = Path(__file__).resolve().parent.parent.parent.parent
            model_path = str(project_root / "models" / "audio" / "resnet18_scream_detector_v2.pth")
        
        self.model_path = model_path
        
        # ìƒ˜í”Œ ë°ì´í„° ê²½ë¡œ ì„¤ì •
        if sample_data_path is None:
            self.sample_data_path = Path(__file__).resolve().parent / "sample_data"
        else:
            self.sample_data_path = Path(sample_data_path)
        
        # ëª¨ë¸ ë¡œë“œ (lazy loading)
        self._scream_model = None
        self._stt_model = None
        
        # VAD í•„í„° (Silero VAD ì‚¬ìš©)
        self._vad_filter = None
        try:
            from sentinel_pipeline.infrastructure.audio.processors.vad_filter import create_vad_filter
            self._vad_filter = create_vad_filter(
                sample_rate=self.SAMPLE_RATE,
                threshold=0.5,  # ì¤‘ê°„ ì„ê³„ê°’ (ë¹„ëª…ë„ í†µê³¼ì‹œí‚¤ë©´ì„œ ì¡ìŒì€ ê±°ë¦„)
            )
            if self._vad_filter:
                logger.info("ğŸ›¡ï¸ Silero VAD Filter Initialized (Gatekeeper)")
        except ImportError:
            logger.warning("âš ï¸ silero-vad not installed. VAD filtering disabled. Install with: pip install silero-vad")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize VAD filter: {e}")
        
        # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ìºì‹œ - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        # ë¹„ëª…: scream_*.mp3 (ì˜ˆ: scream_1.mp3, scream_2.mp3, scream_3.mp3)
        # ê¸´ê¸‰ í‚¤ì›Œë“œ: ê²½ì°°.m4a, ê¸´ê¸‰.m4a, ë„ì™€ì£¼ì„¸ìš”.m4a, ì‚¬ëŒì‚´ë ¤.m4a, ì‚´ë ¤ì£¼ì„¸ìš”.m4a
        # ì¼ë°˜: non_scream_*.wav, ë§ˆì´í¬.m4a, ìŒì„±íŒŒì¼.m4a, ì²˜ë¦¬ìš©ëŸ‰.m4a, í…ŒìŠ¤íŠ¸.m4a
        self._all_audio_files: list[tuple[str, np.ndarray, str]] = []  # [(filename, audio_data, category), ...]
        
        # ê¸´ê¸‰ í‚¤ì›Œë“œ íŒŒì¼ ëª©ë¡ (í•˜ë“œì½”ë”©)
        self.EMERGENCY_KEYWORD_FILES = {"ê²½ì°°.m4a", "ê¸´ê¸‰.m4a", "ë„ì™€ì£¼ì„¸ìš”.m4a", "ì‚¬ëŒì‚´ë ¤.m4a", "ì‚´ë ¤ì£¼ì„¸ìš”.m4a"}
        self.NORMAL_M4A_FILES = {"ë§ˆì´í¬.m4a", "ìŒì„±íŒŒì¼.m4a", "ì²˜ë¦¬ìš©ëŸ‰.m4a", "í…ŒìŠ¤íŠ¸.m4a"}
        
        # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ (í•„ìˆ˜)
        self._load_sample_audio_files()
        
        if not self._all_audio_files:
            raise ValueError(f"ìƒ˜í”Œ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.sample_data_path}")
        
    def _load_models(self):
        """ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ)"""
        if self._scream_model is None:
            logger.info("ScreamDetector ëª¨ë¸ ë¡œë”© ì¤‘...")
            from sentinel_pipeline.infrastructure.audio.processors.scream_detector import ScreamDetector
            
            self._scream_model = ScreamDetector(
                model_path=self.model_path,
                threshold=self.scream_threshold,
                device=self.device,
                enable_filtering=True,  # ResNet-ScreamDetectì™€ ë™ì¼í•˜ê²Œ í•„í„°ë§ í™œì„±í™”
            )
            logger.info(f"ScreamDetector ë¡œë“œ ì™„ë£Œ: {self.device}")
            
        if self._stt_model is None:
            # large ëª¨ë¸ ì„ íƒ ì‹œ large-v3-turbo ì‚¬ìš© (ResNet í”„ë¡œì íŠ¸ì™€ ë™ì¼)
            model_name = self.whisper_model_name
            if model_name == "large":
                model_name = "large-v3-turbo"
            
            logger.info(f"Whisper STT ëª¨ë¸ ë¡œë”© ì¤‘ ({model_name})...")
            from faster_whisper import WhisperModel
            
            # GPU ì‚¬ìš© ì‹œ float16, CPUëŠ” int8
            compute_type = "float16" if self.device == "cuda" else "int8"
            
            self._stt_model = WhisperModel(
                model_name,
                device=self.device,
                compute_type=compute_type,
            )
            
            # ëª¨ë¸ì´ ì‹¤ì œë¡œ GPUì— ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            if self.device == "cuda":
                import torch
                # ë” ìƒì„¸í•œ GPU ë©”ëª¨ë¦¬ ì •ë³´ ë¡œê¹…
                allocated_mb = torch.cuda.memory_allocated() / (1024 * 1024)
                reserved_mb = torch.cuda.memory_reserved() / (1024 * 1024)
                logger.info(f"   GPU Memory Allocated: {allocated_mb:.2f} MB, Reserved: {reserved_mb:.2f} MB")
                if allocated_mb == 0:
                    logger.warning("   GPUì— ë©”ëª¨ë¦¬ê°€ í• ë‹¹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"Whisper ë¡œë“œ ì™„ë£Œ: {model_name} on {self.device}")
    
    def _load_sample_audio_files(self):
        """sample_data í´ë”ì—ì„œ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜"""
        if not self.sample_data_path.exists():
            logger.warning(f"ìƒ˜í”Œ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {self.sample_data_path}")
            return
        
        logger.info(f"ìƒ˜í”Œ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì¤‘: {self.sample_data_path}")
        
        # ì¹´í…Œê³ ë¦¬ ì¹´ìš´í„°
        category_counts = {"scream": 0, "emergency_keyword": 0, "normal": 0}
        
        # ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ (wav, m4a, mp3)
        all_files = (
            list(self.sample_data_path.glob("*.wav")) + 
            list(self.sample_data_path.glob("*.m4a")) + 
            list(self.sample_data_path.glob("*.mp3"))
        )
        
        for file_path in all_files:
            try:
                audio, sr = librosa.load(str(file_path), sr=self.SAMPLE_RATE)
                
                # íŒŒì¼ëª… ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                filename = file_path.name
                category = self._classify_audio_file(filename)
                
                self._all_audio_files.append((filename, audio, category))
                category_counts[category] += 1
                logger.debug(f"  ë¡œë“œ: {filename} ({len(audio)/self.SAMPLE_RATE:.1f}ì´ˆ) [{category}]")
            except Exception as e:
                logger.warning(f"  ë¡œë“œ ì‹¤íŒ¨: {file_path.name} - {e}")
        
        logger.info(f"ìƒ˜í”Œ ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ: ë¹„ëª… {category_counts['scream']}ê°œ, "
                   f"ê¸´ê¸‰í‚¤ì›Œë“œ {category_counts['emergency_keyword']}ê°œ, "
                   f"ì¼ë°˜ {category_counts['normal']}ê°œ")
    
    def _classify_audio_file(self, filename: str) -> str:
        """
        íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        
        Returns:
            "scream" | "emergency_keyword" | "normal"
        """
        # ë¹„ëª…: scream_*.mp3 (ì˜ˆ: scream_1.mp3, scream_2.mp3, scream_3.mp3)
        if filename.startswith("scream_"):
            return "scream"
        # ê¸´ê¸‰ í‚¤ì›Œë“œ: íŠ¹ì • í•œêµ­ì–´ íŒŒì¼ë“¤
        elif filename in self.EMERGENCY_KEYWORD_FILES:
            return "emergency_keyword"
        # ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜
        else:
            return "normal"
    
    def _get_random_audio(self) -> tuple[str, np.ndarray, str]:
        """
        ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì™„ì „ ëœë¤ ì„ íƒ
        
        Returns:
            (íŒŒì¼ëª…, ì˜¤ë””ì˜¤ ë°ì´í„°, ì¹´í…Œê³ ë¦¬) íŠœí”Œ
            ì¹´í…Œê³ ë¦¬: "scream" | "emergency_keyword" | "normal"
        """
        return random.choice(self._all_audio_files)
    
    def _prepare_audio_chunk(self, audio: np.ndarray, prefer_start: bool = False) -> np.ndarray:
        """
        ì˜¤ë””ì˜¤ë¥¼ 2ì´ˆ ìœˆë„ìš°ë¡œ ìë¥´ê±°ë‚˜ íŒ¨ë”©
        
        Args:
            audio: ì›ë³¸ ì˜¤ë””ì˜¤ ë°ì´í„°
            prefer_start: Trueë©´ ì²˜ìŒë¶€í„° ì‹œì‘ (ë¹„ëª… íŒŒì¼ì˜ ê²½ìš° ìœ ìš©)
            
        Returns:
            2ì´ˆ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ì²­í¬
        """
        target_len = int(self.SAMPLE_RATE * self.WINDOW_SEC)
        
        if len(audio) < target_len:
            # ì§§ìœ¼ë©´ íŒ¨ë”©
            return np.pad(audio, (0, target_len - len(audio)), mode='constant')
        elif len(audio) > target_len:
            if prefer_start:
                # ì²˜ìŒë¶€í„° ì‹œì‘ (ë¹„ëª… íŒŒì¼ì˜ ê²½ìš° ì²˜ìŒì— ë¹„ëª…ì´ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
                return audio[:target_len]
            else:
                # ê¸¸ë©´ ëœë¤ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸° (ì¼ë°˜ì ì¸ ê²½ìš°)
                start = random.randint(0, len(audio) - target_len)
                return audio[start:start + target_len]
        else:
            return audio
    
    
    def warmup(self):
        """
        ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡ ì€ ëŠë¦¬ë¯€ë¡œ ì¸¡ì • ì „ ì‹¤í–‰)
        
        ë©˜í†  ì¡°ì–¸: ì²« ë²ˆì§¸ ì‹¤í–‰(Inference)ì€ í•­ìƒ ëŠë¦½ë‹ˆë‹¤.
        í…ŒìŠ¤íŠ¸ ì „ì— í•œ ë²ˆ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ëŒë ¤ì£¼ëŠ” Warm-up ë¡œì§ì´
        ì½”ë“œì— í¬í•¨ë˜ì–´ì•¼ ì •í™•í•œ ì¸¡ì •ì´ ë©ë‹ˆë‹¤.
        """
        logger.info("ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
        
        self._load_models()
        
        # ì‹¤ì œ ì˜¤ë””ì˜¤ë¡œ ì›Œë°ì—…
        _, warmup_audio, _ = self._get_random_audio()
        warmup_chunk = self._prepare_audio_chunk(warmup_audio)
        
        # ScreamDetector ì›Œë°ì—…
        for _ in range(3):
            self._scream_model.predict(warmup_chunk)
        
        # Whisper STT ì›Œë°ì—…
        for _ in range(2):
            segments, _ = self._stt_model.transcribe(
                warmup_chunk,
                beam_size=1,
                language="ko",
            )
            # ì œë„ˆë ˆì´í„° ì†Œë¹„
            list(segments)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "cuda":
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
        
        logger.info("ì›Œë°ì—… ì™„ë£Œ!")
    
    def _measure_resources(self) -> tuple[float, float, float]:
        """í˜„ì¬ ì‹œì ì˜ GPU ë©”ëª¨ë¦¬, CPU ì‚¬ìš©ë¥ , ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì¸¡ì •"""
        gpu_mem = 0.0
        if self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.synchronize()
            gpu_mem = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
        
        cpu_pct = psutil.cpu_percent(interval=None)
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
        mem_info = psutil.virtual_memory()
        system_mem_mb = mem_info.used / (1024 * 1024)  # MB
        
        return gpu_mem, cpu_pct, system_mem_mb
    
    def simulate_stream(
        self,
        stream_id: int,
    ) -> StreamMetrics:
        """
        ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ì˜ ì²˜ë¦¬ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        
        Args:
            stream_id: ìŠ¤íŠ¸ë¦¼ ì‹ë³„ì
            
        Returns:
            StreamMetrics: ì²˜ë¦¬ ê²°ê³¼ ë©”íŠ¸ë¦­ (ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í¬í•¨)
        """
        start_time = time.perf_counter()
        
        # ì˜¤ë””ì˜¤ ì„ íƒ (ì‹¤ì œ íŒŒì¼ì—ì„œ ëœë¤)
        audio_filename, raw_audio, audio_category = self._get_random_audio()
        # ë¹„ëª… íŒŒì¼ì˜ ê²½ìš° ì²˜ìŒë¶€í„° ì‹œì‘ (ë¹„ëª…ì´ ì²˜ìŒì— ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
        prefer_start = (audio_category == "scream")
        audio = self._prepare_audio_chunk(raw_audio, prefer_start=prefer_start)
        
        # --- Step 0: VAD í•„í„° (CPU ì²˜ë¦¬ - ë§¤ìš° ë¹ ë¦„) ---
        # ëª©ì†Œë¦¬ë‚˜ ê°•í•œ ì†Œë¦¬ê°€ ì—†ìœ¼ë©´ ì•„ì˜ˆ Drop (GPU ì‚¬ìš© 0)
        if self._vad_filter and not self._vad_filter.is_speech(audio):
            # ì¡°ìš©í•œ êµ¬ê°„ì´ë‚˜ ì¡ìŒë§Œ ìˆìŒ -> ì²˜ë¦¬ ì¢…ë£Œ
            # ë¡œê·¸ ë ˆë²¨ì„ Debugë¡œ ë‚®ì¶°ì„œ ë„ë°° ë°©ì§€
            logger.debug(f"Stream {stream_id}: ğŸ”‡ Silence/Noise dropped by VAD")
            
            # VADë¡œ ì°¨ë‹¨ëœ ê²½ìš°ì—ë„ ë©”íŠ¸ë¦­ ë°˜í™˜ (ì²˜ë¦¬ ì‹œê°„ì€ ë§¤ìš° ì§§ìŒ)
            total_time = time.perf_counter() - start_time
            gpu_mem, cpu_pct, system_mem = self._measure_resources()
            
            return StreamMetrics(
                stream_id=stream_id,
                step1_latency=0.0,  # VADì—ì„œ ì°¨ë‹¨ë˜ì–´ ì²˜ë¦¬ ì•ˆ í•¨
                step2_latency=0.0,
                total_latency=total_time,
                detected=False,
                scream_prob=0.0,
                transcript="",
                audio_file=audio_filename,
                audio_category=audio_category,
                gpu_memory_mb=gpu_mem,
                cpu_percent=cpu_pct,
                system_memory_mb=system_mem,
            )
        
        # --- Step 1: ë¹„ëª… ê°ì§€ (ResNet18) ---
        t1_start = time.perf_counter()
        result = self._scream_model.predict(audio)
        
        if self.device == "cuda":
            torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            
        t1_end = time.perf_counter()
        
        scream_prob = result.get("prob", 0.0)
        status = result.get("status", "UNKNOWN")
        reason = result.get("reason", "")
        
        # ë””ë²„ê¹…: ë¹„ëª… íŒŒì¼ì¸ë° probê°€ 0ì´ë©´ ë¡œê·¸ ì¶œë ¥
        if audio_category == "scream" and scream_prob < 0.01:
            logger.warning(
                f"Stream {stream_id}: ë¹„ëª… íŒŒì¼({audio_filename})ì¸ë° prob={scream_prob:.4f}, "
                f"status={status}, reason={reason}"
            )
        
        is_scream = result.get("is_scream", False) or scream_prob > self.scream_threshold
        
        # --- Step 2: STT (Whisper) - ë¹„ëª…ì´ ì•„ë‹ ë•Œë§Œ ì‹¤í–‰ ---
        # ë¹„ëª… ê°ì§€ ì‹œ: ì¦‰ê° ì•Œë¦¼ (ë³„ë„ API) â†’ STT ë¶ˆí•„ìš”
        # ë¹„ëª… ì•„ë‹ ì‹œ: ìŒì„± ë‚´ìš© ë¶„ì„ â†’ STTë¡œ í‚¤ì›Œë“œ ê²€ì¶œ
        t2_latency = 0.0
        transcript = ""
        
        if not is_scream:
            t2_start = time.perf_counter()
            
            segments, _ = self._stt_model.transcribe(
                audio,
                beam_size=5,
                language="ko",
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=200, threshold=0.3),
            )
            
            # ì œë„ˆë ˆì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_parts = [s.text for s in segments]
            transcript = " ".join(text_parts).strip()
            
            if self.device == "cuda":
                torch.cuda.synchronize()
                
            t2_end = time.perf_counter()
            t2_latency = t2_end - t2_start
        
        total_time = time.perf_counter() - start_time
        
        # ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ì‹œì ì˜ ë¦¬ì†ŒìŠ¤ ì¸¡ì •
        gpu_mem, cpu_pct, system_mem = self._measure_resources()
        
        return StreamMetrics(
            stream_id=stream_id,
            step1_latency=t1_end - t1_start,
            step2_latency=t2_latency,
            total_latency=total_time,
            detected=is_scream,
            scream_prob=scream_prob,
            transcript=transcript,
            audio_file=audio_filename,
            audio_category=audio_category,
            gpu_memory_mb=gpu_mem,
            cpu_percent=cpu_pct,
            system_memory_mb=system_mem,
        )
    
    def run_batch_test(
        self,
        warmup: bool = True,
        progress_callback: Optional[Callable[[dict], None]] = None,
    ) -> BenchmarkResult:
        """
        Nê°œì˜ ìŠ¤íŠ¸ë¦¼ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜ (Batch Processing)
        
        Args:
            warmup: ì›Œë°ì—… ì‹¤í–‰ ì—¬ë¶€
            progress_callback: ê° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°± (ì‹¤ì‹œê°„ ë¡œê·¸ìš©)
            
        Returns:
            BenchmarkResult: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        # ëª¨ë¸ ë¡œë“œ ë‹¨ê³„ ì•Œë¦¼
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": "ëª¨ë¸ ë¡œë”© ì¤‘...",
                "phase": "loading"
            })
        
        # ëª¨ë¸ ë¡œë“œ ë° ì›Œë°ì—…
        self._load_models()
        
        if warmup:
            if progress_callback:
                progress_callback({
                    "type": "status", 
                    "message": "ì›Œë°ì—… ì‹¤í–‰ ì¤‘...",
                    "phase": "warmup"
                })
            self.warmup()
        
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": f"í…ŒìŠ¤íŠ¸ ì‹œì‘: {self.num_streams} ìŠ¤íŠ¸ë¦¼",
                "phase": "running"
            })
        
        # CPU ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        psutil.cpu_percent(interval=None)  # ì´ˆê¸°í™”
        
        # GPU ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘
        if self.device == "cuda":
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            gc.collect()
        
        results: list[StreamMetrics] = []
        scream_count = 0
        stt_count = 0
        
        loop_start = time.perf_counter()
        
        # ì‹œë®¬ë ˆì´ì…˜: Nê°œì˜ ìŠ¤íŠ¸ë¦¼ì„ ìˆœì°¨ ì²˜ë¦¬ (ì‹¤ì œ ì˜¤ë””ì˜¤ ëœë¤ ì„ íƒ)
        for i in range(self.num_streams):
            metrics = self.simulate_stream(i)
            results.append(metrics)
            
            if metrics.detected:
                scream_count += 1
            else:
                # STTëŠ” ë¹„ëª…ì´ ì•„ë‹ ë•Œ ì‹¤í–‰ë¨
                stt_count += 1
            
            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± (ë§¤ ìŠ¤íŠ¸ë¦¼ë§ˆë‹¤)
            if progress_callback:
                progress_callback({
                    "type": "stream_result",
                    "stream_id": i,
                    "total_streams": self.num_streams,
                    "audio_file": metrics.audio_file,
                    "audio_category": metrics.audio_category,
                    "detected": metrics.detected,
                    "scream_prob": round(metrics.scream_prob, 3),
                    "step1_latency": round(metrics.step1_latency * 1000, 2),
                    "step2_latency": round(metrics.step2_latency * 1000, 2),
                    "total_latency": round(metrics.total_latency * 1000, 2),
                    "transcript": metrics.transcript,
                    "gpu_memory_mb": round(metrics.gpu_memory_mb, 2),
                    "cpu_percent": round(metrics.cpu_percent, 1),
                    "system_memory_mb": round(metrics.system_memory_mb, 2),
                })
            
            # ì§„í–‰ ìƒí™© ë¡œê¹… (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0 or i == self.num_streams - 1:
                logger.debug(f"ì§„í–‰: {i + 1}/{self.num_streams} ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì™„ë£Œ")
        
        loop_end = time.perf_counter()
        total_time = loop_end - loop_start
        
        # CPU ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
        cpu_percent = psutil.cpu_percent(interval=None)
        
        # GPU ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ
        if self.device == "cuda":
            torch.cuda.synchronize()
            gpu_mem = torch.cuda.memory_allocated() / 1024**2
            gpu_mem_peak = torch.cuda.max_memory_allocated() / 1024**2
        else:
            gpu_mem = 0
            gpu_mem_peak = 0
        
        # í†µê³„ ì§‘ê³„
        latencies = [r.total_latency for r in results]
        avg_latency = float(np.mean(latencies))
        max_latency = float(np.max(latencies))
        min_latency = float(np.min(latencies))
        fps = self.num_streams / total_time if total_time > 0 else 0
        
        return BenchmarkResult(
            streams=self.num_streams,
            avg_latency=round(avg_latency, 4),
            max_latency=round(max_latency, 4),
            min_latency=round(min_latency, 4),
            fps=round(fps, 2),
            gpu_memory_mb=round(gpu_mem, 2),
            gpu_memory_peak_mb=round(gpu_mem_peak, 2),
            cpu_percent=round(cpu_percent, 1),
            device=self.device,
            scream_count=scream_count,
            stt_count=stt_count,
            total_time=round(total_time, 3),
            details=[
                {
                    "stream_id": r.stream_id,
                    "chunk_id": 0,  # Batch ëª¨ë“œì—ì„œëŠ” ê° ìŠ¤íŠ¸ë¦¼ë‹¹ 1ê°œ ì²­í¬
                    "timestamp": loop_start + r.total_latency,  # ì²˜ë¦¬ ì™„ë£Œ ì‹œì 
                    "step1_latency": round(r.step1_latency * 1000, 2),  # msë¡œ ë³€í™˜
                    "step2_latency": round(r.step2_latency * 1000, 2),
                    "total_latency": round(r.total_latency * 1000, 2),
                    "detected": r.detected,
                    "scream_prob": round(r.scream_prob, 3),
                    "audio_file": r.audio_file,
                    "audio_category": r.audio_category,
                    "gpu_memory_mb": round(r.gpu_memory_mb, 2),
                    "cpu_percent": round(r.cpu_percent, 1),
                    "system_memory_mb": round(r.system_memory_mb, 2),
                    "transcript": r.transcript,
                }
                for r in results
            ],
        )
    
    def run_continuous_test(
        self,
        duration: float = 30.0,
        interval: float = 1.0,
        warmup: bool = True,
        progress_callback: Optional[Callable[[dict], None]] = None,
    ) -> BenchmarkResult:
        """
        ì‹¤ì œ ìŠ¤íŠ¸ë¦¼ì²˜ëŸ¼ ì§€ì†ì ì¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        
        ê° ìŠ¤íŠ¸ë¦¼ì´ interval ê°„ê²©ìœ¼ë¡œ ê³„ì† ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ë§ˆì¹˜ Nê°œì˜ ë§ˆì´í¬ê°€ ë™ì‹œì— ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ë¥¼ ë³´ë‚´ëŠ” ê²ƒì²˜ëŸ¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
        
        Args:
            duration: í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)
            interval: ê° ìŠ¤íŠ¸ë¦¼ì˜ ì˜¤ë””ì˜¤ ì…ë ¥ ê°„ê²© (ì´ˆ)
            warmup: ì›Œë°ì—… ì‹¤í–‰ ì—¬ë¶€
            progress_callback: ê° ì²­í¬ ì²˜ë¦¬ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°± (ì‹¤ì‹œê°„ ë¡œê·¸ìš©)
            
        Returns:
            BenchmarkResult: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        import threading
        import queue
        
        # ëª¨ë¸ ë¡œë“œ ë‹¨ê³„ ì•Œë¦¼
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": "ëª¨ë¸ ë¡œë”© ì¤‘...",
                "phase": "loading"
            })
        
        # ëª¨ë¸ ë¡œë“œ ë° ì›Œë°ì—…
        self._load_models()
        
        if warmup:
            if progress_callback:
                progress_callback({
                    "type": "status",
                    "message": "ì›Œë°ì—… ì‹¤í–‰ ì¤‘...",
                    "phase": "warmup"
                })
            self.warmup()
        
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": f"ì—°ì† í…ŒìŠ¤íŠ¸ ì‹œì‘: {self.num_streams} ìŠ¤íŠ¸ë¦¼ x {duration}ì´ˆ",
                "phase": "running"
            })
        
        logger.info(f"ì—°ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {self.num_streams} streams x {duration}ì´ˆ, ê°„ê²©={interval}ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥ìš©
        results_queue: queue.Queue = queue.Queue()
        stop_event = threading.Event()
        
        # CPU/GPU ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘
        if self.device == "cuda":
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            gc.collect()
        
        cpu_samples = []
        
        # ì½œë°±ìš© lock (thread-safe)
        callback_lock = threading.Lock()
        
        def stream_worker(stream_id: int):
            """ê°œë³„ ìŠ¤íŠ¸ë¦¼ ì›Œì»¤ - ì§€ì†ì ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ì‹¤ì œ íŒŒì¼ ëœë¤ ì„ íƒ)"""
            chunk_count = 0
            first_chunk = True
            worker_start_time = time.perf_counter()
            
            if progress_callback:
                with callback_lock:
                    progress_callback({
                        "type": "status",
                        "message": f"Stream {stream_id} ì›Œì»¤ í™œì„±í™”ë¨. ì²« ì²­í¬ ì²˜ë¦¬ ì‹œì‘...",
                        "phase": "running"
                    })
            
            while not stop_event.is_set():
                # ì˜¤ë””ì˜¤ ì²˜ë¦¬
                try:
                    chunk_start = time.perf_counter()
                    
                    if first_chunk and progress_callback:
                        with callback_lock:
                            progress_callback({
                                "type": "status",
                                "message": f"Stream {stream_id} ì²« ì²­í¬ ì²˜ë¦¬ ì¤‘... (ì˜ˆìƒ ì‹œê°„: CPU ëª¨ë“œì—ì„œ 30-60ì´ˆ)",
                                "phase": "running"
                            })
                    
                    metrics = self.simulate_stream(stream_id)
                    chunk_end = time.perf_counter()
                    chunk_duration = chunk_end - chunk_start
                    
                    metrics_dict = {
                        "stream_id": stream_id,
                        "chunk_id": chunk_count,
                        "step1_latency": round(metrics.step1_latency * 1000, 2),
                        "step2_latency": round(metrics.step2_latency * 1000, 2),
                        "total_latency": round(metrics.total_latency * 1000, 2),
                        "detected": metrics.detected,
                        "scream_prob": round(metrics.scream_prob, 3),
                        "audio_file": metrics.audio_file,
                        "audio_category": metrics.audio_category,
                        "gpu_memory_mb": round(metrics.gpu_memory_mb, 2),
                        "cpu_percent": round(metrics.cpu_percent, 1),
                        "system_memory_mb": round(metrics.system_memory_mb, 2),
                        "transcript": metrics.transcript,
                        "timestamp": time.time(),
                    }
                    results_queue.put(metrics_dict)
                    
                    # ì²« ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ì•Œë¦¼
                    if first_chunk and progress_callback:
                        with callback_lock:
                            progress_callback({
                                "type": "status",
                                "message": f"âœ… Stream {stream_id} ì²« ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {chunk_duration:.1f}ì´ˆ)",
                                "phase": "running"
                            })
                        first_chunk = False
                    
                    # ì‹¤ì‹œê°„ ì½œë°± (thread-safe)
                    if progress_callback:
                        with callback_lock:
                            progress_callback({
                                "type": "stream_result",
                                "stream_id": stream_id,
                                "chunk_id": chunk_count,
                                "total_streams": self.num_streams,
                                **metrics_dict
                            })
                    
                    chunk_count += 1
                except Exception as e:
                    logger.error(f"Stream {stream_id} ì˜¤ë¥˜: {e}", exc_info=True)
                    if progress_callback:
                        with callback_lock:
                            progress_callback({
                                "type": "error",
                                "message": f"Stream {stream_id} ì˜¤ë¥˜: {str(e)}"
                            })
                
                # ë‹¤ìŒ ì˜¤ë””ì˜¤ê¹Œì§€ ëŒ€ê¸°
                time.sleep(interval)
        
        # ìŠ¤íŠ¸ë¦¼ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        threads = []
        test_start = time.perf_counter()
        workers_started = 0
        
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": f"{self.num_streams}ê°œ ìŠ¤íŠ¸ë¦¼ ì›Œì»¤ ì‹œì‘ ì¤‘...",
                "phase": "running"
            })
        
        for i in range(self.num_streams):
            t = threading.Thread(target=stream_worker, args=(i,), daemon=True)
            t.start()
            threads.append(t)
            workers_started += 1
            
            if progress_callback:
                progress_callback({
                    "type": "status",
                    "message": f"Stream {i} ì›Œì»¤ ì‹œì‘ë¨ ({workers_started}/{self.num_streams})",
                    "phase": "running"
                })
            
            # ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ì‹œê°„ ë¶„ì‚° (ë™ì‹œ ì‹œì‘ ë°©ì§€)
            time.sleep(interval / self.num_streams)
        
        logger.info(f"ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨. {duration}ì´ˆ ë™ì•ˆ ì‹¤í–‰...")
        
        if progress_callback:
            progress_callback({
                "type": "status",
                "message": f"âœ… ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì›Œì»¤ ì‹œì‘ ì™„ë£Œ ({self.num_streams}ê°œ). ì²« ì²­í¬ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘...",
                "phase": "running"
            })
        
        # duration ë™ì•ˆ ëŒ€ê¸°í•˜ë©´ì„œ CPU ìƒ˜í”Œë§
        elapsed = 0
        last_processed = 0
        last_log_time = test_start
        
        while elapsed < duration:
            time.sleep(1.0)
            elapsed = time.perf_counter() - test_start
            cpu_samples.append(psutil.cpu_percent(interval=None))
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            processed = results_queue.qsize()
            logger.debug(f"ì§„í–‰: {elapsed:.0f}/{duration:.0f}ì´ˆ, ì²˜ë¦¬ëœ ì²­í¬: {processed}")
            
            # ì²˜ë¦¬ ì¤‘ì¸ ìŠ¤íŠ¸ë¦¼ ìˆ˜ ì¶”ì • (ì²˜ë¦¬ ì†ë„ ê¸°ë°˜)
            if processed > last_processed:
                # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
                time_diff = elapsed - (last_log_time - test_start) if last_log_time > test_start else 1.0
                rate = (processed - last_processed) / max(time_diff, 0.1)  # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰
            else:
                rate = 0
            
            last_processed = processed
            last_log_time = time.perf_counter()
            
            # ì§„í–‰ë¥  ì½œë°± (ì²˜ë¦¬ëœ ì²­í¬ê°€ ì—†ì–´ë„ ì‹œê°„ ê²½ê³¼ëŠ” í‘œì‹œ)
            if progress_callback:
                # ì²˜ë¦¬ ì¤‘ì¸ ì›Œì»¤ ìˆ˜ ì¶”ì • (í™œì„± ìŠ¤ë ˆë“œ ìˆ˜)
                active_threads = sum(1 for t in threads if t.is_alive())
                
                progress_callback({
                    "type": "progress",
                    "elapsed": round(elapsed, 1),
                    "duration": duration,
                    "processed": processed,
                    "processing_rate": round(rate, 1),
                    "active_workers": active_threads,
                    "total_workers": self.num_streams,
                    "percent": round(elapsed / duration * 100, 1),
                    "note": "ì²˜ë¦¬ ì¤‘..." if processed == 0 and elapsed < duration else None
                })
        
        # í…ŒìŠ¤íŠ¸ ì¢…ë£Œ
        stop_event.set()
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        for t in threads:
            t.join(timeout=2.0)
        
        test_end = time.perf_counter()
        total_time = test_end - test_start
        
        # ê²°ê³¼ ìˆ˜ì§‘
        all_results = []
        while not results_queue.empty():
            try:
                all_results.append(results_queue.get_nowait())
            except queue.Empty:
                break
        
        # GPU ë©”ëª¨ë¦¬ ì¸¡ì •
        if self.device == "cuda":
            torch.cuda.synchronize()
            gpu_mem = torch.cuda.memory_allocated() / 1024**2
            gpu_mem_peak = torch.cuda.max_memory_allocated() / 1024**2
        else:
            gpu_mem = 0
            gpu_mem_peak = 0
        
        # í†µê³„ ì§‘ê³„
        if all_results:
            latencies = [r["total_latency"] for r in all_results]
            avg_latency = float(np.mean(latencies)) / 1000  # ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
            max_latency = float(np.max(latencies)) / 1000
            min_latency = float(np.min(latencies)) / 1000
            
            scream_count = sum(1 for r in all_results if r["detected"])
            stt_count = sum(1 for r in all_results if not r["detected"])
        else:
            avg_latency = max_latency = min_latency = 0
            scream_count = stt_count = 0
        
        fps = len(all_results) / total_time if total_time > 0 else 0
        avg_cpu = float(np.mean(cpu_samples)) if cpu_samples else 0
        
        logger.info(f"ì—°ì† í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(all_results)} ì²­í¬ ì²˜ë¦¬, {fps:.1f} chunks/sec")
        
        return BenchmarkResult(
            streams=self.num_streams,
            avg_latency=round(avg_latency, 4),
            max_latency=round(max_latency, 4),
            min_latency=round(min_latency, 4),
            fps=round(fps, 2),
            gpu_memory_mb=round(gpu_mem, 2),
            gpu_memory_peak_mb=round(gpu_mem_peak, 2),
            cpu_percent=round(avg_cpu, 1),
            device=self.device,
            scream_count=scream_count,
            stt_count=stt_count,
            total_time=round(total_time, 3),
            duration=duration,
            total_processed=len(all_results),
            details=all_results,
        )
    
    def get_system_status(self) -> dict[str, Any]:
        """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        status = {
            "device": self.device,
            "cpu_percent": psutil.cpu_percent(interval=0.1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_available_gb": round(psutil.virtual_memory().available / 1024**3, 2),
        }
        
        if self.device == "cuda" and torch.cuda.is_available():
            status.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total_mb": round(torch.cuda.get_device_properties(0).total_memory / 1024**2, 0),
                "gpu_memory_allocated_mb": round(torch.cuda.memory_allocated() / 1024**2, 2),
                "gpu_memory_cached_mb": round(torch.cuda.memory_reserved() / 1024**2, 2),
            })
        
        return status
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            gc.collect()
        
        self._scream_model = None
        self._stt_model = None
        self._all_audio_files = []
        
        logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def main():
    """CLI ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„° (ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©)")
    parser.add_argument("--streams", "-n", type=int, default=10, help="ìŠ¤íŠ¸ë¦¼ ê°œìˆ˜ (ê¸°ë³¸: 10)")
    parser.add_argument("--whisper-model", "-m", type=str, default="base", help="Whisper ëª¨ë¸ (ê¸°ë³¸: base)")
    parser.add_argument("--cpu-only", action="store_true", help="CPUë§Œ ì‚¬ìš©")
    parser.add_argument("--no-warmup", action="store_true", help="ì›Œë°ì—… ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--continuous", "-c", action="store_true", help="ì—°ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‹¤ì œ ìŠ¤íŠ¸ë¦¼ì²˜ëŸ¼)")
    parser.add_argument("--duration", "-t", type=float, default=30.0, help="ì—°ì† í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 30)")
    parser.add_argument("--interval", "-i", type=float, default=1.0, help="ì˜¤ë””ì˜¤ ì…ë ¥ ê°„ê²© (ì´ˆ, ê¸°ë³¸: 1.0)")
    parser.add_argument("--output", "-o", type=str, default=None, help="ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: benchmark_result_YYYYMMDD_HHMMSS.csv)")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    from sentinel_pipeline.common.logging import configure_logging
    log_level = "DEBUG" if args.verbose else "INFO"
    configure_logging(level=log_level)
    
    print()
    print("=" * 60)
    print("  GPU Load Test Simulator")
    print("=" * 60)
    print(f"  Mode: {'Continuous (Real Stream Simulation)' if args.continuous else 'Batch (One-shot)'}")
    print(f"  Streams: {args.streams}")
    print(f"  Audio Source: Real Files (sample_data/)")
    if args.continuous:
        print(f"  Duration: {args.duration}ì´ˆ")
        print(f"  Interval: {args.interval}ì´ˆ (ê° ìŠ¤íŠ¸ë¦¼)")
    print(f"  Whisper Model: {args.whisper_model}")
    print(f"  Device: {'CPU' if args.cpu_only else 'GPU (if available)'}")
    print("=" * 60)
    print()
    
    # ì‹œë®¬ë ˆì´í„° ìƒì„± ë° ì‹¤í–‰
    sim = LoadTestSimulator(
        num_streams=args.streams,
        gpu_enabled=not args.cpu_only,
        whisper_model=args.whisper_model,
    )
    
    # ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥
    status = sim.get_system_status()
    print(f"[System] Device: {status['device']}")
    print(f"[System] CPU: {status['cpu_percent']}%")
    print(f"[System] Memory: {status['memory_percent']}% used")
    if "gpu_name" in status:
        print(f"[System] GPU: {status['gpu_name']}")
        print(f"[System] VRAM: {status['gpu_memory_allocated_mb']:.0f} / {status['gpu_memory_total_mb']:.0f} MB")
    print()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    if args.continuous:
        print(f"[Test] ì—°ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({args.duration}ì´ˆ)...")
        result = sim.run_continuous_test(
            duration=args.duration,
            interval=args.interval,
            warmup=not args.no_warmup,
        )
    else:
        print("[Test] ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        result = sim.run_batch_test(
            warmup=not args.no_warmup,
        )
    
    # ê²°ê³¼ ì¶œë ¥
    print()
    print("=" * 60)
    print("  Benchmark Results")
    print("=" * 60)
    print(f"  Mode: {'Continuous' if args.continuous else 'Batch'}")
    print(f"  Total Streams: {result.streams}")
    if args.continuous:
        print(f"  Duration: {result.duration:.0f} sec")
        print(f"  Total Chunks Processed: {result.total_processed}")
    print(f"  Scream Detected: {result.scream_count} | STT Executed: {result.stt_count}")
    print("-" * 60)
    print(f"  Avg Latency: {result.avg_latency * 1000:.1f} ms")
    print(f"  Max Latency: {result.max_latency * 1000:.1f} ms")
    print(f"  Min Latency: {result.min_latency * 1000:.1f} ms")
    print(f"  Throughput: {result.fps:.1f} {'chunks' if args.continuous else 'streams'}/sec")
    print("-" * 60)
    print(f"  GPU Memory: {result.gpu_memory_mb:.0f} MB")
    print(f"  GPU Peak Memory: {result.gpu_memory_peak_mb:.0f} MB")
    print(f"  CPU Usage: {result.cpu_percent:.1f}%")
    print(f"  Total Time: {result.total_time:.2f} sec")
    print("=" * 60)
    
    # ì‹¤ì œ ì˜¤ë””ì˜¤ ì‚¬ìš© ì‹œ ìƒì„¸ ê²°ê³¼ ì¶œë ¥
    if args.use_real_audio and args.verbose:
        print()
        print("  Stream Details:")
        print("-" * 60)
        for d in result.details:
            status = "ğŸš¨ SCREAM" if d["detected"] else "âœ… SAFE"
            category = d.get("audio_category", "normal")
            category_emoji = {"scream": "ğŸ”´ë¹„ëª…", "emergency_keyword": "ğŸŸ ê¸´ê¸‰", "normal": "ğŸŸ¢ì¼ë°˜"}.get(category, "âšª")
            # ë¹„ëª… ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš°ì—ë§Œ ì •ë‹µ ì—¬ë¶€ ì²´í¬
            is_scream_gt = category == "scream"
            correct = "âœ“" if d["detected"] == is_scream_gt else "âœ—"
            print(f"  [{d['stream_id']:2d}] {d['audio_file']:25s} | GT:{category_emoji} | {status} (prob:{d['scream_prob']:.2f}) {correct}")
            if d["transcript"]:
                print(f"       â””â”€ STT: \"{d['transcript']}\"")
        print("=" * 60)
        
        # ì •í™•ë„ ê³„ì‚° (ë¹„ëª… ê°ì§€ ì •í™•ë„: scream ì¹´í…Œê³ ë¦¬ë§Œ detected=Trueì—¬ì•¼ í•¨)
        correct_count = sum(1 for d in result.details if d["detected"] == (d.get("audio_category") == "scream"))
        accuracy = correct_count / len(result.details) * 100 if result.details else 0
        print(f"  Detection Accuracy: {correct_count}/{len(result.details)} ({accuracy:.1f}%)")
        print("=" * 60)
    
    # CSV ì €ì¥
    csv_path = result.save_to_csv(args.output)
    print()
    print(f"[Save] ê²°ê³¼ê°€ CSV íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_path}")
    
    # ì •ë¦¬
    sim.cleanup()


if __name__ == "__main__":
    main()
