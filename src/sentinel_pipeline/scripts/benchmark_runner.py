"""
GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„° (Batch & Async ì§€ì› ë²„ì „)

- BatchScreamDetector (GPU) ê¸°ë°˜ ë¹„ë™ê¸° ì²˜ë¦¬
- GlobalInferenceEngine (GPU) ê¸°ë°˜ Whisper STT ì—°ë™
- SSE ì—°ë™ì„ ìœ„í•œ Progress Callback ì§€ì›
"""

import asyncio
import csv
import time
import psutil
import torch
import numpy as np
import logging
import threading
from typing import List, Dict, Optional, Callable
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime

from sentinel_pipeline.infrastructure.audio.processors.batch_scream_detector import BatchScreamDetector
from sentinel_pipeline.infrastructure.audio.processors.risk_analyzer import GlobalInferenceEngine
from sentinel_pipeline.scripts.data_loader import AudioDataLoader

logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """API í˜¸í™˜ ê²°ê³¼ë¥¼ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤"""
    streams: int
    avg_latency: float  # ì´ˆ ë‹¨ìœ„ (APIì—ì„œ msë¡œ ë³€í™˜)
    max_latency: float
    min_latency: float
    fps: float
    gpu_memory_mb: float
    gpu_memory_peak_mb: float
    cpu_percent: float
    device: str
    scream_count: int
    stt_count: int
    total_time: float
    duration: float = 0.0
    total_processed: int = 0
    details: List[Dict] = field(default_factory=list)
    
    def save_to_csv(self, filepath: Optional[str] = None) -> str:
        """
        ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filepath: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"benchmark_result_{timestamp}.csv"
        
        filepath = Path(filepath)
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            
            # í—¤ë” ì‘ì„±
            writer.writerow([
                "stream_id",
                "chunk_id",
                "timestamp",
                "audio_file",
                "audio_category",
                "detected",
                "scream_prob",
                "step1_latency_ms",
                "step2_latency_ms",
                "total_latency_ms",
                "gpu_memory_mb",
                "cpu_percent",
                "system_memory_mb",
                "transcript",
            ])
            
            # ë°ì´í„° ì‘ì„±
            for d in self.details:
                writer.writerow([
                    d.get("stream_id", ""),
                    d.get("chunk_id", 0),
                    d.get("timestamp", ""),
                    d.get("audio_file", ""),
                    d.get("audio_category", "normal"),
                    d.get("detected", False),
                    round(d.get("scream_prob", 0.0), 3),
                    round(d.get("step1_latency", 0.0), 2),
                    round(d.get("step2_latency", 0.0), 2),
                    round(d.get("total_latency", 0.0), 2),
                    round(d.get("gpu_memory_mb", 0.0), 2),
                    round(d.get("cpu_percent", 0.0), 1),
                    round(d.get("system_memory_mb", 0.0), 2),
                    d.get("transcript", ""),
                ])
        
        logger.info(f"ğŸ’¾ Benchmark results saved to {filepath}")
        return str(filepath)

class LoadTestSimulator:
    """
    [GPU-Optimized Load Test Simulator]
    API(benchmark.py)ì™€ scripts/benchmark/runner.pyë¥¼ ì—°ê²°í•˜ëŠ” ê°€êµ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(
        self, 
        num_streams: int = 10, 
        gpu_enabled: bool = True, 
        whisper_model: str = "base",
        batch_size: int = 16
    ):
        self.num_streams = num_streams
        self.device = "cuda" if gpu_enabled and torch.cuda.is_available() else "cpu"
        self.whisper_model_name = whisper_model
        self.batch_size = batch_size
        
        # ëª¨ë¸ ê²½ë¡œ
        project_root = Path(__file__).resolve().parent.parent.parent.parent
        self.scream_model_path = str(project_root / "models" / "audio" / "resnet18_scream_detector_v2.pth")
        
        # ì—”ì§„ (Lazy Loading)
        self.detector: Optional[BatchScreamDetector] = None
        self.stt_engine: Optional[GlobalInferenceEngine] = None
        self.data_loader = AudioDataLoader()
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = []
        self._lock = threading.Lock()
        
        # ë¹„ë™ê¸° ë£¨í”„ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë  ë•Œ í•„ìš”)
        self._loop = None

    def _ensure_engines(self):
        """ì—”ì§„ ì´ˆê¸°í™” (ë™ê¸° í˜¸ì¶œìš©)"""
        if self.detector is None:
            # BatchScreamDetectorëŠ” ë‚´ë¶€ì ìœ¼ë¡œ loopê°€ í•„ìš”í•˜ë¯€ë¡œ 
            # í˜„ì¬ ìŠ¤ë ˆë“œì— ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê±°ë‚˜ get_event_loop ì‚¬ìš©
            try:
                self._loop = asyncio.get_event_loop()
            except RuntimeError:
                self._loop = asyncio.new_event_loop()
                asyncio.set_event_loop(self._loop)
            
            self.detector = BatchScreamDetector(
                model_path=self.scream_model_path,
                device=self.device,
                batch_size=self.batch_size
            )
            
            # start()ê°€ asyncì´ë¯€ë¡œ run_until_completeë¡œ ì‹¤í–‰
            if self._loop.is_running():
                # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ create_task ì‚¬ìš©
                asyncio.run_coroutine_threadsafe(self.detector.start(), self._loop)
            else:
                # ë£¨í”„ê°€ ì—†ìœ¼ë©´ run_until_complete ì‚¬ìš©
                self._loop.run_until_complete(self.detector.start())
            
            # Whisper ì—”ì§„ì€ Singleton
            self.stt_engine = GlobalInferenceEngine()
            # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •ì— ë§ì¶° ëª¨ë¸ í¬ê¸° ì¡°ì • ê°€ëŠ¥í•˜ì§€ë§Œ, 
            # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì´ˆê¸°í™”ëœ ì—”ì§„ì„ ì‚¬ìš©í•¨

    async def _process_chunk_async(self, stream_id: int, chunk: np.ndarray, file_info: dict) -> dict:
        """í•œ ê°œì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ë¡œì§"""
        start_t = time.perf_counter()
        # ì²˜ë¦¬ ì‹œì‘  ì‹œê°„ 
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        
        # 1. Scream Detection (Batch)
        s1_start = time.perf_counter()
        scream_res = await self.detector.predict(chunk)
        s1_latency = (time.perf_counter() - s1_start) * 1000.0
        
        # 2. STT (ë¹„ëª…ì´ ì•„ë‹ ë•Œë§Œ ìˆ˜í–‰í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤)
        s2_latency = 0.0
        transcript = ""
        
        if not scream_res['is_scream']:
            s2_start = time.perf_counter()
            # STTëŠ” ë™ê¸° í ë°©ì‹ì´ë¯€ë¡œ ë˜í•‘
            # ë²¤ì¹˜ë§ˆí¬ì´ë¯€ë¡œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë ¤ì•¼ í•¨ (Callback ì‚¬ìš©)
            stt_future = self._loop.create_future()
            
            def stt_callback(res):
                if not stt_future.done():
                    self._loop.call_soon_threadsafe(stt_future.set_result, res)
            
            # RiskAnalyzer êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ê²Œ ìš”ì²­
            from sentinel_pipeline.infrastructure.audio.processors.risk_analyzer import InferenceRequest
            req = InferenceRequest(
                stream_id=f"sim_{stream_id}",
                audio_data=chunk,
                callback=stt_callback,
                timestamp=timestamp
            )
            self.stt_engine.submit(req)
            
            # ê²°ê³¼ ëŒ€ê¸°
            stt_res = await stt_future
            s2_latency = (time.perf_counter() - s2_start) * 1000.0
            transcript = stt_res.get('text', '')

        total_latency = (time.perf_counter() - start_t) * 1000.0
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ (ì²­í¬ ì²˜ë¦¬ ì‹œì )
        gpu_mem = 0
        if "cuda" in self.device:
            gpu_mem = torch.cuda.memory_allocated() / 1024**2
        
        mem_info = psutil.virtual_memory()
        system_mem_mb = mem_info.used / (1024**2)
            
        return {
            "stream_id": stream_id,
            "chunk_id": 0,  # ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” 0, continuous ëª¨ë“œì—ì„œëŠ” chunk_count ì‚¬ìš©
            "timestamp": timestamp,
            "step1_latency": s1_latency,
            "step2_latency": s2_latency,
            "total_latency": total_latency,
            "detected": scream_res['is_scream'],
            "scream_prob": scream_res['prob'],
            "audio_file": file_info.get("filename", "unknown"),
            "audio_category": file_info.get("category", "normal"),
            "gpu_memory_mb": gpu_mem,
            "cpu_percent": psutil.cpu_percent(),
            "system_memory_mb": system_mem_mb,
            "transcript": transcript
        }

    def run_batch_test(self, warmup: bool = True, progress_callback: Callable = None) -> BenchmarkResult:
        """[API ë™ê¸° ì¸í„°í˜ì´ìŠ¤] ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self._ensure_engines()
        return self._loop.run_until_complete(self._run_batch_async(warmup, progress_callback))

    async def _run_batch_async(self, warmup: bool, progress_callback: Callable) -> BenchmarkResult:
        if warmup:
            if progress_callback: progress_callback({"type": "status", "message": "Warming up GPU...", "phase": "warmup"})
            dummy = np.zeros(32000, dtype=np.float32)
            await asyncio.gather(*[self.detector.predict(dummy) for _ in range(self.batch_size)])

        if progress_callback: progress_callback({"type": "status", "message": "Starting batch test...", "phase": "running"})
        
        start_t = time.time()
        tasks = []
        for i in range(self.num_streams):
            file_path, chunk, info = self.data_loader.get_prepared_chunk()
            tasks.append(self._process_chunk_async(i, chunk, info))
        
        # ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘ (ProgressëŠ” ê°œë³„ì ìœ¼ë¡œ ì´ì•¼ í•¨)
        details = []
        for coro in asyncio.as_completed(tasks):
            res = await coro
            details.append(res)
            if progress_callback:
                progress_callback({
                    "type": "stream_result",
                    **res
                })
        
        total_time = time.time() - start_t
        return self._build_result(details, total_time)

    def run_continuous_test(self, duration: float, interval: float, warmup: bool = True, progress_callback: Callable = None) -> BenchmarkResult:
        """[API ë™ê¸° ì¸í„°í˜ì´ìŠ¤] ì—°ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self._ensure_engines()
        return self._loop.run_until_complete(self._run_continuous_async(duration, interval, warmup, progress_callback))

    async def _run_continuous_async(self, duration: float, interval: float, warmup: bool, progress_callback: Callable) -> BenchmarkResult:
        if warmup:
            dummy = np.zeros(32000, dtype=np.float32)
            await self.detector.predict(dummy)

        if progress_callback: progress_callback({"type": "status", "message": f"Starting continuous test ({duration}s)...", "phase": "running"})
        
        start_time = time.time()
        end_time = start_time + duration
        details = []
        processed_count = 0
        
        async def stream_loop(sid):
            nonlocal processed_count
            chunk_count = 0
            while time.time() < end_time:
                filename, chunk, info = self.data_loader.get_prepared_chunk()
                res = await self._process_chunk_async(sid, chunk, info)
                
                # Continuous ëª¨ë“œì—ì„œëŠ” chunk_id ì—…ë°ì´íŠ¸
                res["chunk_id"] = chunk_count
                chunk_count += 1
                
                with self._lock:
                    details.append(res)
                    processed_count += 1
                
                if progress_callback:
                    progress_callback({"type": "stream_result", **res})
                
                # ë‹¤ìŒ ì²­í¬ê¹Œì§€ ëŒ€ê¸°
                await asyncio.sleep(interval)

        # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ë™ì‹œ ì‹¤í–‰
        tasks = [asyncio.create_task(stream_loop(i)) for i in range(self.num_streams)]
        
        # ì§„í–‰ë¥  ì£¼ê¸°ì  ë³´ê³ 
        while time.time() < end_time:
            await asyncio.sleep(1.0)
            if progress_callback:
                elapsed = time.time() - start_time
                progress_callback({
                    "type": "progress",
                    "percent": (elapsed / duration) * 100,
                    "processed": processed_count,
                    "processing_rate": processed_count / elapsed if elapsed > 0 else 0
                })
        
        await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        return self._build_result(details, total_time, continuous=True, duration=duration)

    def _build_result(self, details, total_time, continuous=False, duration=0.0) -> BenchmarkResult:
        if not details:
            return BenchmarkResult(self.num_streams, 0, 0, 0, 0, 0, 0, 0, self.device, 0, 0, total_time)
            
        latencies = [d['total_latency'] for d in details]
        gpu_mems = [d['gpu_memory_mb'] for d in details]
        scream_count = sum(1 for d in details if d['detected'])
        
        return BenchmarkResult(
            streams=self.num_streams,
            avg_latency=np.mean(latencies) / 1000.0,
            max_latency=max(latencies) / 1000.0,
            min_latency=min(latencies) / 1000.0,
            fps=len(details) / total_time,
            gpu_memory_mb=np.mean(gpu_mems),
            gpu_memory_peak_mb=max(gpu_mems),
            cpu_percent=psutil.cpu_percent(),
            device=self.device,
            scream_count=scream_count,
            stt_count=len(details) - scream_count,
            total_time=total_time,
            duration=duration,
            total_processed=len(details),
            details=details
        )

    def get_system_status(self) -> dict:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        mem = psutil.virtual_memory()
        status = {
            "device": self.device,
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": mem.percent,
            "memory_available_gb": mem.available / (1024**3),
            "gpu_name": None,
            "gpu_memory_total_mb": None,
            "gpu_memory_allocated_mb": None,
            "gpu_memory_cached_mb": None
        }
        
        if "cuda" in self.device:
            status["gpu_name"] = torch.cuda.get_device_name(0)
            status["gpu_memory_total_mb"] = torch.cuda.get_device_properties(0).total_memory / 1024**2
            status["gpu_memory_allocated_mb"] = torch.cuda.memory_allocated(0) / 1024**2
            status["gpu_memory_cached_mb"] = torch.cuda.memory_reserved(0) / 1024**2
            
        return status

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.detector:
            asyncio.run_coroutine_threadsafe(self.detector.stop(), self._loop)
            self.detector = None


if __name__ == "__main__":
    """CLI ì‹¤í–‰ë¶€"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„°")
    parser.add_argument("--streams", "-n", type=int, default=10, help="ìŠ¤íŠ¸ë¦¼ ê°œìˆ˜")
    parser.add_argument("--whisper-model", "-m", type=str, default="base", help="Whisper ëª¨ë¸ í¬ê¸°")
    parser.add_argument("--cpu-only", action="store_true", help="CPUë§Œ ì‚¬ìš©")
    parser.add_argument("--no-warmup", action="store_true", help="ì›Œë°ì—… ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--continuous", "-c", action="store_true", help="ì—°ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    parser.add_argument("--duration", "-t", type=float, default=30.0, help="ì—°ì† í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--interval", "-i", type=float, default=1.0, help="ì˜¤ë””ì˜¤ ì…ë ¥ ê°„ê²© (ì´ˆ)")
    parser.add_argument("--output", "-o", type=str, default=None, help="ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s [%(levelname)s] %(message)s')
    
    # ì‹œë®¬ë ˆì´í„° ìƒì„±
    sim = LoadTestSimulator(
        num_streams=args.streams,
        gpu_enabled=not args.cpu_only,
        whisper_model=args.whisper_model,
    )
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        if args.continuous:
            result = sim.run_continuous_test(
                duration=args.duration,
                interval=args.interval,
                warmup=not args.no_warmup
            )
        else:
            result = sim.run_batch_test(
                warmup=not args.no_warmup
            )
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"  Benchmark Results")
        print(f"{'='*60}")
        print(f"  Streams: {result.streams}")
        print(f"  Avg Latency: {result.avg_latency*1000:.1f}ms")
        print(f"  Max Latency: {result.max_latency*1000:.1f}ms")
        print(f"  Min Latency: {result.min_latency*1000:.1f}ms")
        print(f"  Throughput: {result.fps:.1f} {'chunks' if args.continuous else 'streams'}/sec")
        print(f"  GPU Memory: {result.gpu_memory_mb:.0f}MB (Peak: {result.gpu_memory_peak_mb:.0f}MB)")
        print(f"  CPU Usage: {result.cpu_percent:.1f}%")
        print(f"  Scream Detected: {result.scream_count} | STT Executed: {result.stt_count}")
        print(f"{'='*60}\n")
        
        # CSV ì €ì¥
        csv_path = result.save_to_csv(args.output)
        print(f"ğŸ“„ Results saved to: {csv_path}")
        
    finally:
        sim.cleanup()
