"""
ì˜¤ë””ì˜¤ ë§¤ë‹ˆì €

ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì˜ ìƒëª…ì£¼ê¸°ì™€ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import time
import threading
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Dict, Any, Callable
import logging
from pathlib import Path

from sentinel_pipeline.interface.config.schema import AudioStreamConfig
from sentinel_pipeline.domain.models.audio_stream import (
    AudioStreamState,
    AudioStreamStatus,
    AudioStreamStats
)
from sentinel_pipeline.domain.models.event import Event, EventType, EventStage
from sentinel_pipeline.infrastructure.audio.readers.mic_reader import MicAudioReader
from sentinel_pipeline.infrastructure.audio.readers.rtsp_reader import RtspAudioReader
from sentinel_pipeline.infrastructure.audio.processors.batch_scream_detector import BatchScreamDetector
from sentinel_pipeline.infrastructure.audio.processors.risk_analyzer import RiskAnalyzer
from sentinel_pipeline.infrastructure.audio.processors.vad_filter import create_vad_filter
from sentinel_pipeline.interface.api.ws_bus import publish_stream_update, broadcast

logger = logging.getLogger(__name__)


class AudioStreamContext:
    """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
    def __init__(self, config: AudioStreamConfig):
        self.stream_id = config.stream_id
        self.state = AudioStreamState(config=config)
        self.reader = None
        self.risk_analyzer = None
        self.vad_filter = None 
        
        # ìƒíƒœ ì¶”ì 
        self.last_scream_time = 0.0
        self.scream_cooldown = 2.0
        
        # Latest-Win ì „ëµì„ ìœ„í•œ ë²„í¼ ë° ìƒíƒœ
        self.latest_chunk = None  # ê°€ì¥ ìµœì‹  ì˜¤ë””ì˜¤ ì²­í¬ (ëŒ€ê¸°ì—´ í¬ê¸° 1 íš¨ê³¼)
        self.is_processing = False # ë¶„ì„ ë£¨í”„ ì‹¤í–‰ ì¤‘ ì—¬ë¶€
        
        # ì„±ëŠ¥ ì¸¡ì •ìš©
        self.chunk_counter = 0  # ì²­í¬ ì¹´ìš´í„° (Sampling ë¡œê¹…ìš©)
        self.chunk_duration_ms = int(config.chunk_duration * 1000)  # ì²­í¬ í¬ê¸° (ms)


class AudioManager:
    """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë° ë¶„ì„ ê´€ë¦¬ì"""
    
    def __init__(self):
        self._streams: Dict[str, AudioStreamContext] = {}
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="AudioAnalysis")
        self._event_emitter: Optional[EventEmitter] = None
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        
        # ê³µìœ  ë°°ì¹˜ ì—”ì§„ (Lazy Loading)
        self._batch_scream_detector: Optional[BatchScreamDetector] = None
        
        # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
        self.model_dir = Path("models/audio")
        self.scream_model_path = self.model_dir / "resnet18_scream_detector_v2.pth"
        
    def set_dependencies(self, event_emitter: EventEmitter):
        self._event_emitter = event_emitter
        
    def set_event_loop(self, loop: asyncio.AbstractEventLoop):
        self._loop = loop
        # ë£¨í”„ê°€ ì„¤ì •ë˜ë©´ ë°°ì¹˜ ì—”ì§„ë„ ì‹œì‘
        if self._batch_scream_detector:
            self._batch_scream_detector.start(loop)

    def start_stream(self, config: AudioStreamConfig) -> AudioStreamState:
        with self._lock:
            # 1. ê³µìœ  ë°°ì¹˜ íƒì§€ê¸° ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ)
            if self._batch_scream_detector is None:
                model_path = config.scream_model_path if config.scream_model_path else str(self.scream_model_path)
                logger.info(f"ğŸš€ Initializing Shared BatchScreamDetector: {model_path}")
                self._batch_scream_detector = BatchScreamDetector(
                    model_path=model_path,
                    threshold=config.scream_threshold,
                    batch_size=16  # TODO: ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê²Œ ìˆ˜ì • ê°€ëŠ¥
                )
                # ì´ë¯¸ ë£¨í”„ê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì¦‰ì‹œ ì‹œì‘
                if self._loop:
                    self._batch_scream_detector.start(self._loop)
                ctx = self._streams[config.stream_id]
                if ctx.state.is_active:
                    logger.warning(f"Audio stream already active: {config.stream_id}")
                    return ctx.state
                
            ctx = AudioStreamContext(config)
            self._streams[config.stream_id] = ctx
            
            try:
                # 2. í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
                
                if config.stt_enabled:
                    ctx.risk_analyzer = RiskAnalyzer(
                        stream_id=config.stream_id,
                        model_size=config.stt_model_size,
                        enable_medium_path=config.enable_medium_path,
                        enable_heavy_path=config.enable_heavy_path,
                        heavy_path_async=config.heavy_path_async,
                        semantic_threshold=config.semantic_threshold,
                        use_korean_model=config.use_korean_model
                    )
                    
                    try:
                        ctx.vad_filter = create_vad_filter(
                            sample_rate=config.sample_rate,
                            threshold=0.5,
                            use_highpass=True
                        )
                        logger.info(f"ğŸ›¡ï¸ VAD Filter initialized for stream {config.stream_id}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to initialize VAD filter for {config.stream_id}: {e}")
                        ctx.vad_filter = None
                
                # 3. ë¦¬ë” ì´ˆê¸°í™” (ìŠ¤ë ˆë“œ ë¦¬ë” -> ë¹„ë™ê¸° ë¸Œë¦¬ì§€)
                on_chunk_cb = lambda chunk: self._bridge_on_chunk(ctx, chunk)
                
                if config.use_microphone:
                    ctx.reader = MicAudioReader(
                        sample_rate=config.sample_rate,
                        chunk_duration=config.chunk_duration,
                        device_index=config.mic_device_index,
                        on_chunk=on_chunk_cb
                    )
                else:
                    ctx.reader = RtspAudioReader(
                        rtsp_url=config.rtsp_url,
                        sample_rate=config.sample_rate,
                        chunk_duration=config.chunk_duration,
                        on_chunk=on_chunk_cb
                    )
                
                # 3. ì‹œì‘
                ctx.state.set_status(AudioStreamStatus.STARTING)
                try:
                    ctx.reader.start()
                except Exception as e:
                    # ë¦¬ë” ì‹œì‘ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì˜ˆì™¸ ë°œìƒ
                    logger.error(f"Failed to start audio reader for {config.stream_id}: {e}")
                    ctx.state.set_status(AudioStreamStatus.ERROR, str(e))
                    self._notify_status_change(ctx.stream_id, AudioStreamStatus.ERROR)
                    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ APIì—ì„œ ì²˜ë¦¬í•˜ë„ë¡
                
                ctx.state.set_status(AudioStreamStatus.RUNNING)
                self._notify_status_change(ctx.stream_id, AudioStreamStatus.RUNNING)
                logger.info(f"Audio stream started: {config.stream_id}")
                
            except Exception as e:
                # ë‹¤ë¥¸ ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œ ë°œìƒí•œ ì˜ˆì™¸
                logger.error(f"Failed to start audio stream {config.stream_id}: {e}")
                ctx.state.set_status(AudioStreamStatus.ERROR, str(e))
                self._notify_status_change(ctx.stream_id, AudioStreamStatus.ERROR)
                raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ APIì—ì„œ ì²˜ë¦¬í•˜ë„ë¡
                
            return ctx.state

    def stop_stream(self, stream_id: str) -> bool:
        with self._lock:
            if stream_id not in self._streams:
                return False
                
            ctx = self._streams[stream_id]
            
            if ctx.reader:
                ctx.reader.stop()
            
            ctx.state.set_status(AudioStreamStatus.STOPPED)
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            del self._streams[stream_id]
            logger.info(f"Audio stream stopped: {stream_id}")
            
            # ì‚­ì œ ì•Œë¦¼ì€ API ë ˆì´ì–´ì—ì„œ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¼ì´ ì´ë¯¸ ì‚­ì œëœ í›„ì—ëŠ” ìƒíƒœ ë³€ê²½ ì•Œë¦¼ì´ ì˜ë¯¸ ì—†ìŒ)
            return True

    def get_stream_state(self, stream_id: str) -> Optional[AudioStreamState]:
        with self._lock:
            if stream_id in self._streams:
                return self._streams[stream_id].state
            return None

    def get_all_streams(self) -> list[AudioStreamState]:
        with self._lock:
            return [ctx.state for ctx in self._streams.values()]

    def _bridge_on_chunk(self, ctx: AudioStreamContext, chunk: Any):
        """ìŠ¤ë ˆë“œ ë¦¬ë” -> ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ ì—°ê²° ë¸Œë¦¬ì§€"""
        if self._loop and self._loop.is_running():
            asyncio.run_coroutine_threadsafe(
                self._async_on_audio_chunk(ctx, chunk),
                self._loop
            )

    async def _async_on_audio_chunk(self, ctx: AudioStreamContext, chunk: Any):
        """[Async] ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ í•µì‹¬ ë¡œì§"""
        chunk_received_time = time.monotonic()
        
        try:
            ctx.state.stats.record_chunk()
            ctx.chunk_counter += 1
            current_time = time.monotonic()
            
            # 1. ë¹„ëª… ê°ì§€ (Batch Inference ìš”ì²­)
            inference_start_time = time.monotonic()
            
            # ê³µìœ  ë°°ì¹˜ íƒì§€ê¸° í˜¸ì¶œ (awaitë¡œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ë§Œ, ë‹¤ë¥¸ ìŠ¤íŠ¸ë¦¼ì€ ê·¸ ì‚¬ì´ íì— ìŒ“ì„)
            if self._batch_scream_detector:
                scream_result = await self._batch_scream_detector.predict(chunk)
            else:
                return # ì—”ì§„ ë¯¸ì´ˆê¸°í™” ì‹œ ìŠ¤í‚µ
                
            inference_done_time = time.monotonic()
            scream_detected = scream_result['is_scream']
            
            # ì¶”ë¡  ì‹œê°„ ë° ë ˆì´í„´ì‹œ
            inference_ms = (inference_done_time - inference_start_time) * 1000
            total_latency_ms = (inference_done_time - chunk_received_time) * 1000
            
            # WebSocket ìƒíƒœ ì „ì†¡
            self._send_pipeline_status(
                stream_id=ctx.stream_id,
                node_id=2, 
                scream_detected=scream_detected,
                confidence=scream_result.get('prob', 0.0)
            )
            
            if scream_detected:
                if current_time - ctx.last_scream_time > ctx.scream_cooldown:
                    ctx.last_scream_time = current_time
                    ctx.state.stats.scream_detected_count += 1
                    
                    self._send_scream_detected(ctx.stream_id, scream_result['prob'])
                    self._emit_event(
                        stream_id=ctx.stream_id,
                        event_type=EventType.SCREAM,
                        confidence=scream_result['prob'],
                        details={'batch_mode': True}
                    )
                    
                    logger.info(
                        f"[STREAM={ctx.stream_id}] [BATCH] chunk={ctx.chunk_counter} "
                        f"infer={inference_ms:.1f}ms score={scream_result['prob']:.2f} "
                        f"event=DETECTED latency={total_latency_ms:.1f}ms"
                    )
            else:
                if ctx.chunk_counter % 100 == 0:
                    logger.debug(f"[STREAM={ctx.stream_id}] [BATCH] chunk={ctx.chunk_counter} Normal")

                # 2. STT ë° ìœ„í—˜ ë¶„ì„ (VAD í•„í„° í¬í•¨)
                if ctx.risk_analyzer:
                    is_speech = True
                    if ctx.vad_filter:
                        try:
                            # VADëŠ” ì—¬ì „íˆ CPU ì‘ì—… (ì¶”í›„ ì´ê²ƒë„ Batchí™” ê°€ëŠ¥)
                            is_speech = ctx.vad_filter.is_speech(chunk)
                        except Exception:
                            is_speech = True
                    
                    if is_speech:
                        ctx.latest_chunk = chunk
                        if not ctx.is_processing:
                            ctx.is_processing = True
                            # STT ë£¨í”„ëŠ” ì—¬ì „íˆ ThreadPoolì—ì„œ ì‹¤í–‰ (Whisper ê°€ìš©ì„± ë•Œë¬¸)
                            self._executor.submit(self._process_loop, ctx)
                
        except Exception as e:
            logger.error(f"Error in async_on_audio_chunk for {ctx.stream_id}: {e}", exc_info=True)
            ctx.state.stats.record_error()
                
        except Exception as e:
            logger.error(f"Error processing audio chunk for {ctx.stream_id}: {e}")
            ctx.state.stats.record_error()

    def _process_loop(self, ctx: AudioStreamContext):
        """ë¶„ì„ ì‘ì—… ë£¨í”„ (í•­ìƒ ìµœì‹  ì²­í¬ ì²˜ë¦¬)"""
        try:
            while True:
                # 1. ê°€ì¥ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Atomicì— ê°€ê¹Œìš´ ë™ì‘)
                # ë½ì„ ê±¸ì§€ ì•Šì•„ë„ Pythonì˜ ë³€ìˆ˜ í• ë‹¹ì€ Atomicí•˜ì§€ë§Œ, 
                # í™•ì‹¤í•˜ê²Œ ê°€ì ¸ì˜¤ê³  ë¹„ìš°ê¸° ìœ„í•´ ì„ì‹œ ë³€ìˆ˜ ì‚¬ìš©
                chunk = ctx.latest_chunk
                ctx.latest_chunk = None
                
                # ë” ì´ìƒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if chunk is None:
                    ctx.is_processing = False
                    break
                
                # 2. ë¬´ê±°ìš´ ë¶„ì„ ìˆ˜í–‰ (ì—¬ê¸°ì„œ ì‹œê°„ ì†Œìš”)
                # ì´ ë™ì•ˆ ë“¤ì–´ì˜¨ ë°ì´í„°ëŠ” ctx.latest_chunkì— ê³„ì† ë®ì–´ì”Œì›Œì§ (Drop Oldest)
                self._process_risk_analysis(ctx, chunk)
                
                # 3. ë£¨í”„ ë°˜ë³µ -> ê·¸ ì‚¬ì´ì— ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ëŠ”ì§€ í™•ì¸í•˜ëŸ¬ ê°
        except Exception as e:
            logger.error(f"Analysis loop error for {ctx.stream_id}: {e}")
            ctx.is_processing = False

    def _process_risk_analysis(self, ctx: AudioStreamContext, chunk: Any):
        """ë°±ê·¸ë¼ìš´ë“œ STT ì²˜ë¦¬ (ë¹„ë™ê¸° ì½œë°± íŒ¨í„´)"""
        
        # [ë‚´ë¶€ í•¨ìˆ˜] ê²°ê³¼ê°€ ë‚˜ì™”ì„ ë•Œ ì‹¤í–‰ë  ì½œë°± (ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨)
        def on_inference_complete(result: dict):
            # ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ, ì˜ˆì™¸ ì²˜ë¦¬ ì¤‘ìš”
            try:
                # 1. STT ê²°ê³¼ ì „ì†¡
                self._send_stt_result(
                    stream_id=ctx.stream_id,
                    stt_result=result
                )
                
                # 2. ìœ„í—˜ ê°ì§€ ì‹œ ì´ë²¤íŠ¸ ì²˜ë¦¬
                if result.get('is_dangerous'):
                    ctx.state.stats.keyword_detected_count += 1
                    event_type = result.get('event_type') or EventType.CUSTOM
                    
                    self._send_event_detected(
                        stream_id=ctx.stream_id,
                        event_type=event_type,
                        data={
                            'keyword': result.get('keyword'),
                            'text': result.get('text'),
                            'confidence': result.get('confidence')
                        }
                    )
                    
                    self._emit_event(
                        stream_id=ctx.stream_id,
                        event_type=event_type,
                        confidence=result.get('confidence', 0.0),
                        details={
                            'keyword': result.get('keyword'),
                            'text': result.get('text'),
                            'original_text': result.get('text') # í˜¸í™˜ì„±
                        }
                    )
                else:
                    # ì•ˆì „í•  ë•Œë„ ìƒíƒœ ì—…ë°ì´íŠ¸
                    self._send_pipeline_status(
                        stream_id=ctx.stream_id,
                        node_id=5,
                        scream_detected=False,
                        confidence=0.0
                    )
            except Exception as e:
                logger.error(f"Error in inference callback for {ctx.stream_id}: {e}")

        try:
            # RiskAnalyzerì—ê²Œ ì¼ê° ë˜ì§€ê¸° (ì½œë°± í•¨ìˆ˜ë„ ê°™ì´ ì¤Œ)
            ctx.risk_analyzer.process(chunk, callback=on_inference_complete)
                
        except Exception as e:
            logger.error(f"Risk analysis submission error for {ctx.stream_id}: {e}")

    def _emit_event(self, stream_id: str, event_type: EventType, confidence: float, details: dict):
        if self._event_emitter:
            event = Event(
                type=event_type,
                stage=EventStage.DETECTED,
                confidence=confidence,
                stream_id=stream_id,
                module_name="AudioModule",
                details=details
            )
            self._event_emitter.emit(event)

    def _notify_status_change(self, stream_id: str, status: AudioStreamStatus):
        if self._loop:
            payload = {
                "type": "stream_update",  # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” íƒ€ì…
                "stream_id": stream_id,
                "status": status.value
            }
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(publish_stream_update(payload))
            )
    
    def _send_pipeline_status(self, stream_id: str, node_id: int, scream_detected: bool, confidence: float):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡"""
        if self._loop:
            payload = {
                "type": "pipeline_status",
                "stream_id": stream_id,
                "node_id": node_id,
                "scream_detected": scream_detected,
                "confidence": confidence,
                "status": "alert" if scream_detected else "processing"
            }
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(broadcast(payload))
            )
    
    def _send_scream_detected(self, stream_id: str, confidence: float):
        """ë¹„ëª… ê°ì§€ ì´ë²¤íŠ¸ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡"""
        if self._loop:
            payload = {
                "type": "scream_detected",
                "stream_id": stream_id,
                "confidence": confidence
            }
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(broadcast(payload))
            )
    
    def _send_stt_result(self, stream_id: str, stt_result: dict):
        """STT ê²°ê³¼ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡"""
        if self._loop:
            payload = {
                "type": "stt_result",
                "stream_id": stream_id,
                "stt_result": {
                    "text": stt_result.get('text', ''),
                    "risk_analysis": {
                        "is_dangerous": stt_result.get('is_dangerous', False),
                        "keyword": stt_result.get('keyword', ''),
                        "confidence": stt_result.get('confidence', 0.0),
                        "event_type": stt_result.get('event_type', None),
                        "path": stt_result.get('path', 'none')
                    }
                }
            }
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(broadcast(payload))
            )
    
    def _send_event_detected(self, stream_id: str, event_type: EventType, data: dict):
        """ìœ„í—˜ í‚¤ì›Œë“œ ê°ì§€ ì´ë²¤íŠ¸ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡"""
        if self._loop:
            payload = {
                "type": "event_detected",
                "stream_id": stream_id,
                "event_type": event_type.value if hasattr(event_type, 'value') else str(event_type),
                "data": data
            }
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(broadcast(payload))
            )

    def stop_all(self):
        for stream_id in list(self._streams.keys()):
            self.stop_stream(stream_id)
        
        # ê³µìœ  ë°°ì¹˜ íƒì§€ê¸° ì •ë¦¬
        if self._batch_scream_detector and self._loop:
            self._loop.call_soon_threadsafe(
                lambda: asyncio.create_task(self._batch_scream_detector.stop())
            )
            self._batch_scream_detector = None
            
        self._executor.shutdown(wait=False)
